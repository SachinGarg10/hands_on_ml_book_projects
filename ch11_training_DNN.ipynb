{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_DNN\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <span style=\"color:#fff; font-family: 'Bebas Neue'; font-size: 1.2em;\">Number of iterations</span> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in Chapter 10, the backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the **`vanishing gradients`** problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the **`exploding gradients`** problem, which surfaces in recurrent neural networks (see Chapter 15). More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio.1 The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks).***\n",
    "\n",
    "***Looking at the logistic activation function (see Figure 11-1), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), 'b-', linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.grid(True)\n",
    "plt.title('Sigmoid activation function', fontsize=16)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction (please check out the paper if you are interested in the mathematical details). It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in Equation 11-1, where $fan_{avg} = (fan_{in} + fan_{out}/2$. This initialization strategy is called ***Xavier initialization or Glorot initialization***, after the paper’s first author.\n",
    "\n",
    "***Equation 11-1. Glorot initialization (when using the logistic activation function)*** <br>\n",
    "Normal distribution with mean $0$ and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$  \n",
    "Or a uniform distribution between $-r$ and $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you replace $fan_{avg}$ with $fan_{in}$ in Equation 11-1, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it *LeCun initialization*. Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book *Neural Networks*: *Tricks of the Trade* (Springer). LeCun initialization is equivalent to Glorot initialization when $fan_{in} = fan_{out}$. It took over a decade for researchers to realize how important this trick is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning.\n",
    "\n",
    "Some papers have provided similar strategies for different activation functions. These strategies differ only by the scale of the variance and whether they use $fan_{avg}$ or $fan_{in}$, as shown in Table 11-1 (for the uniform distribution, just compute $r = \\sqrt{3\\sigma^2}$). The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called `He initialization`, after the paper’s first author. The SELU activation function will be explained later in this chapter. It should be used with LeCun initialization (preferably with a normal distribution, as we will see)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Initialization parameters of each type of activation function](images/training_DNN/initialization_parameters_activation_func.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses ***Glorot*** initialization with a uniform distribution. When creating a layer, you can change this to ***He*** initialization by setting ***kernel_initializer='he_uniform'*** or ***kernel_initializer='he_normal'*** like this:\n",
    "``` python\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "```\n",
    "\n",
    "If you want ***He*** initialization with a uniform distribution but based on ***fan***$_{avg}$ rather than ***fan***$_{in}$, you can use the ***VarianceScaling*** initializer like this:\n",
    "``` python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fefab146358>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fefaacc0b38>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions\n",
    "\n",
    "One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks—in particular, ***`the ReLU activation function, mostly because it does not saturate for positive values (and because it is fast to compute)`***\n",
    "\n",
    "**Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.**\n",
    "\n",
    "***To solve this problem, you may want to use a variant of the ReLU function, such as the `leaky ReLU`. This function is defined as $LeakyReLU_α(z) = max(αz, z)$ (see Figure 11-2). The hyperparameter $α$ defines how much the function `“leaks”`: it is the slope of the function for $z < 0$ and is typically set to $0.01$. `This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up.` A [2015 paper](https://homl.info/49) `compared several variants of the ReLU activation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function.` In fact, setting $α = 0.2$ (a huge leak) seemed to result in better performance than $α = 0.01$ (a small leak). The paper also evaluated the `randomized leaky ReLU (RReLU)`, where $α$ is picked randomly in a given range during training and is fixed to an average value during testing. `RReLU also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set).` Finally, the paper evaluated the `parametric leaky ReLU (PReLU)`, where $α$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). `PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 4ms/step - loss: 1.8506 - accuracy: 0.4323 - val_loss: 1.0055 - val_accuracy: 0.7110\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.9354 - accuracy: 0.7177 - val_loss: 0.7534 - val_accuracy: 0.7652\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.7437 - accuracy: 0.7588 - val_loss: 0.6670 - val_accuracy: 0.7880\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6584 - accuracy: 0.7847 - val_loss: 0.6064 - val_accuracy: 0.8044\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6166 - accuracy: 0.7983 - val_loss: 0.5686 - val_accuracy: 0.8200\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5751 - accuracy: 0.8113 - val_loss: 0.5419 - val_accuracy: 0.8258\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5471 - accuracy: 0.8201 - val_loss: 0.5200 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5235 - accuracy: 0.8277 - val_loss: 0.5107 - val_accuracy: 0.8306\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5164 - accuracy: 0.8275 - val_loss: 0.4916 - val_accuracy: 0.8380\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4968 - accuracy: 0.8321 - val_loss: 0.4826 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 3ms/step - loss: 1.6975 - accuracy: 0.4971 - val_loss: 0.9259 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.8709 - accuracy: 0.7245 - val_loss: 0.7308 - val_accuracy: 0.7626\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7213 - accuracy: 0.7620 - val_loss: 0.6566 - val_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6449 - accuracy: 0.7881 - val_loss: 0.6005 - val_accuracy: 0.8044\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6078 - accuracy: 0.8003 - val_loss: 0.5657 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5693 - accuracy: 0.8119 - val_loss: 0.5407 - val_accuracy: 0.8236\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5428 - accuracy: 0.8195 - val_loss: 0.5197 - val_accuracy: 0.8316\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5194 - accuracy: 0.8284 - val_loss: 0.5115 - val_accuracy: 0.8308\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5129 - accuracy: 0.8271 - val_loss: 0.4917 - val_accuracy: 0.8376\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4941 - accuracy: 0.8316 - val_loss: 0.4827 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', activation='relu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.plot(z, leaky_relu(z, 0.05), 'b-', linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.grid(True)\n",
    "plt.title('Leaky ReLU activation function', fontsize=16)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the leaky ReLU activation function, create a ***`LeakyReLU`*** layer and add it to your model just after the layer you want to apply it to:\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential([\n",
    "    [...]\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    [...]\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on Fashion MNIST using the Leaky ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 9s 4ms/step - loss: 1.6314 - accuracy: 0.5054 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.8416 - accuracy: 0.7246 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.7053 - accuracy: 0.7637 - val_loss: 0.6427 - val_accuracy: 0.7902\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6325 - accuracy: 0.7907 - val_loss: 0.5900 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5992 - accuracy: 0.8019 - val_loss: 0.5582 - val_accuracy: 0.8202\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5624 - accuracy: 0.8142 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5379 - accuracy: 0.8218 - val_loss: 0.5157 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5152 - accuracy: 0.8294 - val_loss: 0.5079 - val_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5100 - accuracy: 0.8268 - val_loss: 0.4895 - val_accuracy: 0.8388\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4918 - accuracy: 0.8339 - val_loss: 0.4817 - val_accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try PReLU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ***PReLU***, replace ***LeakyRelu(alpha=0.2)*** with ***`PReLU()`***. There is currently no official implementation of RReLU in Keras, but you can fairly easily implement your own (to learn how to do that, see the exercises at the end of Chapter 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 300)               300       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 267,010\n",
      "Trainable params: 267,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7632\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7211 - accuracy: 0.7620 - val_loss: 0.6565 - val_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6448 - accuracy: 0.7880 - val_loss: 0.6003 - val_accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6078 - accuracy: 0.8003 - val_loss: 0.5656 - val_accuracy: 0.8180\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5693 - accuracy: 0.8117 - val_loss: 0.5406 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5428 - accuracy: 0.8193 - val_loss: 0.5196 - val_accuracy: 0.8312\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5193 - accuracy: 0.8284 - val_loss: 0.5113 - val_accuracy: 0.8318\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5129 - accuracy: 0.8274 - val_loss: 0.4916 - val_accuracy: 0.8378\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4941 - accuracy: 0.8313 - val_loss: 0.4826 - val_accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, a [2015 paper](https://homl.info/50) by Djork-Arné Clevert et al. proposed a new activation function called the **`exponential linear unit (ELU)`** that outperformed all the ReLU variants in the authors’ experiments: `training time was reduced, and the neural network performed better on the test set.` Figure 11-3(below) graphs the function, and Equation 11-2 shows its definition.\n",
    "\n",
    "*Equation 11-2. ELU activation function*\n",
    "\n",
    "**ELU**$_\\alpha (z) = \\alpha(exp(z)-1$ **`if`** $z < 0$, $z$ **`if`** $z>= 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure ELU_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.plot(z, elu(z), 'b-', linewidth=2)\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(r'ELU activation function ($\\alpha=1$)', fontsize=16)\n",
    "plt.axis([-4, 4, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"ELU_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELU activation function looks a lot like the ReLU function, with a few major differences:\n",
    "* It takes on negative values when $z < 0$, which allows the unit to have an average output closer to $0$ and helps alleviate the vanishing gradients problem. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter.\n",
    "\n",
    "* It has a nonzero gradient for $z < 0$, which avoids the dead neurons problem.\n",
    "\n",
    "* If $\\alpha$ is equal to $1$ then the function is smooth everywhere, including around $z = 0$, which helps speed up Gradient Descent since it does not bounce as much to the left and right of $z = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f0f611ac4e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a [2017 paper](https://homl.info/selu) by Günter Klambauer et al. introduced the `Scaled ELU (SELU)` activation function: as its name suggests, it is a scaled variant of the ELU activation function. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize: the output of each layer will tend to `preserve a mean of 0 and standard deviation of 1 during training`, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones). There are, however, a few conditions for self-normalization to happen (see the paper for the mathematical justification):\n",
    "\n",
    "* The input features must be standardized (mean 0 and standard deviation 1).\n",
    "\n",
    "* Every hidden layer’s weights must be initialized with LeCun normal initialization. In Keras, this means setting `kernel_initializer=\"lecun_normal\"`.\n",
    "\n",
    "* `The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks (see Chapter 15) or networks with skip connections (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.`\n",
    "\n",
    "* `The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well (see Chapter 14).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ***SELU*** activation, set ***`activation=\"selu\"`*** and ***`kernel_initializer=\"lecun_normal\"`*** when creating a layer:\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(10, activation='selu', \n",
    "                  kernel_initializer='lecun_normal')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f0f5e303eb8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='selu', \n",
    "                  kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***`In Colab notebook`***\n",
    "\n",
    "> This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, `this activation function outperforms the other activation functions very significantly for such neural nets`, so you should really try it out. `Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use` ℓ<sub>1</sub> `or` ℓ<sub>2</sub> `regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize).` ***However, in practice it works quite well with sequential CNNs.*** `If you break self-normalization, SELU will not necessarily outperform other activation functions.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the ***SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SELU is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f0f5fed5ef0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 35s 18ms/step - loss: 1.5515 - accuracy: 0.4109 - val_loss: 1.0267 - val_accuracy: 0.6228\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.8707 - accuracy: 0.6588 - val_loss: 0.7100 - val_accuracy: 0.7198\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.7340 - accuracy: 0.7023 - val_loss: 0.6401 - val_accuracy: 0.7432\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.6358 - accuracy: 0.7548 - val_loss: 0.5612 - val_accuracy: 0.8010\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.5677 - accuracy: 0.7932 - val_loss: 0.5534 - val_accuracy: 0.8102\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now look at what happens if we try to use the ReLU activation function instead:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\",\n",
    "                             kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\",\n",
    "                                 kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 33s 16ms/step - loss: 2.0794 - accuracy: 0.1857 - val_loss: 1.2931 - val_accuracy: 0.4426\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.2462 - accuracy: 0.4646 - val_loss: 0.8471 - val_accuracy: 0.6764\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 1.1994 - accuracy: 0.5252 - val_loss: 0.8759 - val_accuracy: 0.6806\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.9132 - accuracy: 0.6373 - val_loss: 0.7527 - val_accuracy: 0.7110\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 0.7970 - accuracy: 0.6848 - val_loss: 0.7049 - val_accuracy: 0.7310\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`Not great at all, we suffered from the vanishing/exploding gradients problem.`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIP\n",
    "\n",
    "> So, which activation function should you use for the hidden layers of your deep neural networks? Although your mileage will vary, in general **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic</span>**. If the network’s architecture prevents it from self-normalizing, then **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ELU</span>** may perform better than **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">SELU</span>** (since **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">SELU</span>** is not smooth at $z = 0$). If you care a lot about runtime latency, then you may prefer **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">leaky ReLU</span>**. If you don’t want to tweak yet another hyperparameter, you may use the default $α$ values used by Keras (e.g., $0.3$ for **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">leaky ReLU</span>**). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">RReLU</span>** if your network is overfitting or **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">PReLU</span>** if you have a huge training set. That said, because **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ReLU</span>** is the most used activation function (by far), many libraries and hardware accelerators provide **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ReLU</span>**-specific optimizations; therefore, if speed is your priority, **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ReLU</span>** might still be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although using _He_ initialization along with *ELU* (or any variant of *ReLU*) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [2015 paper](https://homl.info/51), Sergey Ioffe and Christian Szegedy proposed a technique called `Batch Normalization (BN) that addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”). The whole operation is summarized step by step in Equation 11-3.\n",
    "\n",
    "![Batch Normalization algorithm](images/training_DNN/batch_normalization_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm:\n",
    "\n",
    "* $μ_B$ is the vector of input means, evaluated over the whole mini-batch $B$ (it contains one mean per input).\n",
    "\n",
    "* $σ_B$ is the vector of input standard deviations, also evaluated over the whole mini-batch (it contains one standard deviation per input).\n",
    "\n",
    "* $m_B$ is the number of instances in the mini-batch.\n",
    "\n",
    "* $\\hat{x}^{(i)}$ is the vector of zero-centered and normalized inputs for instance $i$.\n",
    "\n",
    "* $γ$ is the output scale parameter vector for the layer (it contains one scale parameter per input).\n",
    "\n",
    "* $⊗$ represents element-wise multiplication (each input is multiplied by its corresponding output scale parameter).\n",
    "\n",
    "* $β$ is the output shift (offset) parameter vector for the layer (it contains one offset parameter per input). Each input is offset by its corresponding shift parameter.\n",
    "\n",
    "* $ε$ is a tiny number that avoids division by zero (typically $10^{–5}$). This is called a `smoothing` term.\n",
    "\n",
    "* $z^{(i)}$ is the output of the BN operation. It is a rescaled and shifted version of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it’s not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable. `One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions. However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer’s input means and standard deviations.` This is what Keras does automatically when you use the BatchNormalization layer. To sum up, four parameter vectors are learned in each batch-normalized layer: $\\gamma$ (the output scale vector) and $\\beta$ (the output offset vector) are learned through regular backpropagation, and μ (the final input mean vector) and $\\sigma$ (the final input standard deviation vector) are estimated using an exponential moving average. Note that $\\mu$ and $\\sigma$ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations in Equation 11-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all the deep neural networks they experimented with, leading to a huge improvement in the ImageNet classification task (ImageNet is a large database of images classified into many classes, commonly used to evaluate computer vision systems). The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function. The networks were also much less sensitive to the weight initialization. The authors were able to use much larger learning rates, significantly speeding up the learning process. Specifically, they note that:\n",
    "\n",
    "> ***Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. […] Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Finally, like a gift that keeps on giving, Batch Normalization acts like a regularizer, reducing the need for other regularization techniques (such as dropout, described later in this chapter).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it’s often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. For example, if the previous layer computes $XW + b$, then the BN layer will compute $γ⊗(XW + b – μ)/σ + β$ (ignoring the smoothing term $ε$ in the denominator). If we define $W′ = γ⊗W/σ$ and $b′ = γ⊗(b – μ)/σ + β$, the equation simplifies to $XW′ + b′$. So if we replace the previous layer’s weights and biases ($W$ and $b$) with the updated weights and biases ($W′$ and $b′$), we can get rid of the BN layer (TFLite’s optimizer does this automatically; see Chapter 19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "> You may find that training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance. All in all, `wall time` will usually be shorter (this is the time measured by the clock on your wall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a BatchNormalization layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model. For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s all! **`In this tiny example with just two hidden layers, it’s unlikely that Batch Normalization will have a very positive impact; but for deeper networks it can make a tremendous difference.`**\n",
    "\n",
    "Let’s display the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 334,346\n",
      "Trainable params: 331,578\n",
      "Non-trainable params: 2,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each BN layer adds four parameters per input: $γ$, $β$, $μ$, and $σ$ (for example, the first BN layer adds $3,136$ parameters, which is $4 × 784$). The last two parameters, $μ$ and $σ$, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable” (if you count the total number of BN parameters, $3,136 + 1,200 + 400$, and divide by $2$, you get $2,368$, which is the total number of non-trainable parameters in this model).\n",
    "\n",
    "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_3/gamma:0', True),\n",
       " ('batch_normalization_3/beta:0', True),\n",
       " ('batch_normalization_3/moving_mean:0', False),\n",
       " ('batch_normalization_3/moving_variance:0', False)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you create a BN layer in Keras, it also creates **`two operations` that will be called by Keras at each iteration during training. These operations will update the moving averages.** Since we are using the TensorFlow backend, these operations are TensorFlow operations (we will discuss TF operations in Chapter 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bn1.updates #deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 6ms/step - loss: 1.0867 - accuracy: 0.6377 - val_loss: 0.5053 - val_accuracy: 0.8268\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.5625 - accuracy: 0.8011 - val_loss: 0.4401 - val_accuracy: 0.8492\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.5007 - accuracy: 0.8237 - val_loss: 0.4117 - val_accuracy: 0.8588\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4586 - accuracy: 0.8373 - val_loss: 0.3937 - val_accuracy: 0.8612\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4427 - accuracy: 0.8436 - val_loss: 0.3822 - val_accuracy: 0.8648\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4132 - accuracy: 0.8521 - val_loss: 0.3730 - val_accuracy: 0.8704\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4003 - accuracy: 0.8578 - val_loss: 0.3646 - val_accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3829 - accuracy: 0.8647 - val_loss: 0.3613 - val_accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3808 - accuracy: 0.8637 - val_loss: 0.3541 - val_accuracy: 0.8736\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3642 - accuracy: 0.8692 - val_loss: 0.3505 - val_accuracy: 0.8744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra**<br>\n",
    "***Testing different activation function and batch normalization layer:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "keras.layers.BatchNormalization(),\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", \n",
    "                             kernel_initializer='he_normal'))\n",
    "keras.layers.BatchNormalization(), # commented when testing on without BN layer\n",
    "\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", \n",
    "                                kernel_initializer='he_normal'))\n",
    "    keras.layers.BatchNormalization(), # commented when testing on without BN layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 30s 16ms/step - loss: 2.3026 - accuracy: 0.1004 - val_loss: 2.3026 - val_accuracy: 0.0914\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 2.3026 - accuracy: 0.0992 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 2.3026 - accuracy: 0.0985 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 2.3026 - accuracy: 0.1030 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3027 - val_accuracy: 0.0914\n"
     ]
    }
   ],
   "source": [
    "# with batch normalization layer + when kernel_initializer not given\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 33s 17ms/step - loss: 2.0873 - accuracy: 0.2029 - val_loss: 1.7030 - val_accuracy: 0.3480\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 1.7249 - accuracy: 0.3299 - val_loss: 1.3305 - val_accuracy: 0.4888\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 27s 15ms/step - loss: 1.3244 - accuracy: 0.4778 - val_loss: 1.3690 - val_accuracy: 0.4374\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 1.0806 - accuracy: 0.5655 - val_loss: 0.8215 - val_accuracy: 0.6950\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.0400 - accuracy: 0.5943 - val_loss: 1.2865 - val_accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "# with batch normalization layer with kernel_initializer='he_normal'\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 31s 16ms/step - loss: 2.0873 - accuracy: 0.2029 - val_loss: 1.7030 - val_accuracy: 0.3480\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.7249 - accuracy: 0.3299 - val_loss: 1.3305 - val_accuracy: 0.4888\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.3244 - accuracy: 0.4778 - val_loss: 1.3690 - val_accuracy: 0.4374\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.0806 - accuracy: 0.5655 - val_loss: 0.8215 - val_accuracy: 0.6950\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.0400 - accuracy: 0.5943 - val_loss: 1.2865 - val_accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "# without batch normalization layer expect after input layer  with kernel_initializer='he_normal'\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on SELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 35s 18ms/step - loss: 1.5515 - accuracy: 0.4109 - val_loss: 1.0267 - val_accuracy: 0.6228\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.8707 - accuracy: 0.6588 - val_loss: 0.7100 - val_accuracy: 0.7198\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.7340 - accuracy: 0.7023 - val_loss: 0.6401 - val_accuracy: 0.7432\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.6358 - accuracy: 0.7548 - val_loss: 0.5612 - val_accuracy: 0.8010\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.5677 - accuracy: 0.7932 - val_loss: 0.5534 - val_accuracy: 0.8102\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after (as we just did). There is some debate about this, as which is preferable seems to depend on the task—you can experiment with this too to see which option works best on your dataset. To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass use_bias=False when creating it):`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes applying BN before the activation function works better (there's a debate on this topic). Moreover, the layer before a `BatchNormalization` layer does not need to have bias terms, since the `BatchNormalization` layer some as well, it would be a waste of parameters, so you can set `use_bias=False` when creating those layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 17s 7ms/step - loss: 1.2672 - accuracy: 0.5757 - val_loss: 0.6053 - val_accuracy: 0.8032\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.6409 - accuracy: 0.7833 - val_loss: 0.5076 - val_accuracy: 0.8298\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.5577 - accuracy: 0.8090 - val_loss: 0.4632 - val_accuracy: 0.8456\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.5063 - accuracy: 0.8237 - val_loss: 0.4370 - val_accuracy: 0.8486\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4859 - accuracy: 0.8297 - val_loss: 0.4195 - val_accuracy: 0.8548\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4584 - accuracy: 0.8389 - val_loss: 0.4054 - val_accuracy: 0.8590\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4426 - accuracy: 0.8447 - val_loss: 0.3949 - val_accuracy: 0.8616\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4251 - accuracy: 0.8496 - val_loss: 0.3871 - val_accuracy: 0.8676\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4188 - accuracy: 0.8541 - val_loss: 0.3797 - val_accuracy: 0.8662\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4062 - accuracy: 0.8556 - val_loss: 0.3726 - val_accuracy: 0.8694\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called [Gradient Clipping](https://homl.info/52). ***`This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs`***, as we will see in Chapter 15. `For other types of networks, BN is usually sufficient.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7f0fe3e72860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimizer will clip every component of the gradient vector to a value between $–1.0$ and $1.0$. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between $–1.0$ and $1.0$. The threshold is a hyperparameter you can tune. Note that it may change the orientation of the gradient vector. For instance, if the original gradient vector is $[0.9, 100.0]$, it points mostly in the direction of the second axis; but once you clip it by value, you get $[0.9, 1.0]$, which points roughly in the diagonal between the two axes. In practice, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting ***`clipnorm` instead of `clipvalue`***. This will clip the whole gradient if its $ℓ_2$ norm is greater than the threshold you picked. For example, if you set $clipnorm=1.0$, then the vector $[0.9, 100.0]$ will be clipped to $[0.00899964, 0.9999595]$, preserving its orientation but almost eliminating the first component. If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Keras optimizers accept `clipnorm` or `clipvalue` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle (we will discuss how to find them in Chapter 14), then reuse the lower layers of this network. This technique is called ***transfer learning***. It will not only speed up training considerably, but also require significantly less training data.\n",
    "\n",
    "Suppose you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network (see Figure 11-4).\n",
    "\n",
    "![Reusing pretrained layers](images/training_DNN/reusing_pretrained_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "> If the input pictures of your task don't have the same size as the ones used in the original task, you will usually have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work best when the inputs have similar low-level features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
    "<br><br>\n",
    "Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIP\n",
    "> `The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing hte output layer.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Try freezing all the reused layers first (i.e., make their weights non-trainable so that Gradient Descent won’t modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.`\n",
    "<br><br>\n",
    "`If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at an example. Suppose the Fashion MNIST dataset only contained eight classes—for example, all the classes except for sandal and shirt. Someone built and trained a Keras model on that set and got reasonably good performance (>90% accuracy). Let’s call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (positive=shirt, negative=sandal). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let’s call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it’s a much easier task (there are just two classes), you were hoping for more. While drinking your morning coffee, you realize that your task is quite similar to task A, so perhaps transfer learning can help? Let’s find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_y_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_y_6]\n",
    "    y_A[y_A > 6] -= 2 # class indicies 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_y_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_y_6], y_A), (X[y_5_or_y_6], y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 28, 28), (55000,), (5000, 28, 28), (5000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43986, 28, 28), (43986,), (4014, 28, 28), (4014,), (8000, 28, 28), (8000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape, y_train_A.shape, X_valid_A.shape, y_valid_A.shape, X_test_A.shape, y_test_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 28, 28), (200,), (986, 28, 28), (986,), (2000, 28, 28), (2000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape, y_train_B.shape, X_valid_B.shape, y_valid_B.shape, X_test_B.shape, y_test_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "model_A.add(keras.layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss='sparse_categorical_crossentropy',\n",
    "               optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.9249 - accuracy: 0.6994 - val_loss: 0.3896 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3651 - accuracy: 0.8745 - val_loss: 0.3288 - val_accuracy: 0.8827\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3182 - accuracy: 0.8896 - val_loss: 0.3014 - val_accuracy: 0.8986\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3049 - accuracy: 0.8953 - val_loss: 0.2896 - val_accuracy: 0.9011\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2804 - accuracy: 0.9028 - val_loss: 0.2775 - val_accuracy: 0.9061\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2701 - accuracy: 0.9078 - val_loss: 0.2736 - val_accuracy: 0.9066\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 6s 5ms/step - loss: 0.2626 - accuracy: 0.9092 - val_loss: 0.2718 - val_accuracy: 0.9091\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2609 - accuracy: 0.9120 - val_loss: 0.2589 - val_accuracy: 0.9143\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 6s 5ms/step - loss: 0.2557 - accuracy: 0.9109 - val_loss: 0.2560 - val_accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2511 - accuracy: 0.9136 - val_loss: 0.2542 - val_accuracy: 0.9160\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 6s 5ms/step - loss: 0.2431 - accuracy: 0.9168 - val_loss: 0.2495 - val_accuracy: 0.9148\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2422 - accuracy: 0.9168 - val_loss: 0.2513 - val_accuracy: 0.9123\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2360 - accuracy: 0.9178 - val_loss: 0.2444 - val_accuracy: 0.9158\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2266 - accuracy: 0.9232 - val_loss: 0.2414 - val_accuracy: 0.9173\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2225 - accuracy: 0.9235 - val_loss: 0.2448 - val_accuracy: 0.9195\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2261 - accuracy: 0.9215 - val_loss: 0.2386 - val_accuracy: 0.9188\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 6s 5ms/step - loss: 0.2191 - accuracy: 0.9250 - val_loss: 0.2409 - val_accuracy: 0.9175\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2171 - accuracy: 0.9255 - val_loss: 0.2427 - val_accuracy: 0.9155\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2180 - accuracy: 0.9246 - val_loss: 0.2329 - val_accuracy: 0.9200\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2112 - accuracy: 0.9268 - val_loss: 0.2334 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, validation_data=(X_valid_A, y_valid_A), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save('model_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "model_B.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 48ms/step - loss: 0.5977 - accuracy: 0.7285 - val_loss: 0.5192 - val_accuracy: 0.7911\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.4927 - accuracy: 0.7552 - val_loss: 0.4383 - val_accuracy: 0.8408\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 72ms/step - loss: 0.3959 - accuracy: 0.8526 - val_loss: 0.3794 - val_accuracy: 0.8773\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3442 - accuracy: 0.8824 - val_loss: 0.3344 - val_accuracy: 0.9026\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2879 - accuracy: 0.9228 - val_loss: 0.2994 - val_accuracy: 0.9168\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2637 - accuracy: 0.9520 - val_loss: 0.2712 - val_accuracy: 0.9290\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2263 - accuracy: 0.9714 - val_loss: 0.2482 - val_accuracy: 0.9341\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.2186 - accuracy: 0.9732 - val_loss: 0.2286 - val_accuracy: 0.9483\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1836 - accuracy: 0.9932 - val_loss: 0.2116 - val_accuracy: 0.9594\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1906 - accuracy: 0.9881 - val_loss: 0.1983 - val_accuracy: 0.9615\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1638 - accuracy: 0.9963 - val_loss: 0.1864 - val_accuracy: 0.9604\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1610 - accuracy: 0.9950 - val_loss: 0.1761 - val_accuracy: 0.9645\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1391 - accuracy: 0.9981 - val_loss: 0.1664 - val_accuracy: 0.9675\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1261 - accuracy: 0.9973 - val_loss: 0.1583 - val_accuracy: 0.9675\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1308 - accuracy: 0.9931 - val_loss: 0.1503 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1240 - accuracy: 0.9963 - val_loss: 0.1439 - val_accuracy: 0.9696\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1089 - accuracy: 0.9963 - val_loss: 0.1380 - val_accuracy: 0.9706\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1088 - accuracy: 0.9950 - val_loss: 0.1324 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1006 - accuracy: 0.9981 - val_loss: 0.1273 - val_accuracy: 0.9706\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0926 - accuracy: 0.9981 - val_loss: 0.1231 - val_accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to load model A and create a new model based on that model’s layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"model_A.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s reuse all the layers except for the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Note that model_A and model_B_on_A now share some layers. When you train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone model_A before you reuse its layers. To do this, you clone model A’s architecture with clone.model(), then copy its weights (since clone_model() does not clone the weights):`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 8)                 408       \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A_clone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer’s trainable attribute to False and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_B_on_A.compile(loss='binary_crossentropy', \n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3), \n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE**\n",
    "> `You must always compile your model after you freeze or unfreeze layers.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 42ms/step - loss: 0.1999 - accuracy: 0.9533 - val_loss: 0.2160 - val_accuracy: 0.9665\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1983 - accuracy: 0.9604 - val_loss: 0.2105 - val_accuracy: 0.9665\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1815 - accuracy: 0.9674 - val_loss: 0.2052 - val_accuracy: 0.9716\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1877 - accuracy: 0.9606 - val_loss: 0.2004 - val_accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2 and previously we used 1e-3\n",
    "optimizer = keras.optimizers.SGD(lr=1e-3) # the default lr is 1e-2 and previously we used 1e-3\n",
    "model_B_on_A.compile(loss='binary_crossentropy', \n",
    "                    optimizer=optimizer, \n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 43ms/step - loss: 0.1733 - accuracy: 0.9533 - val_loss: 0.1753 - val_accuracy: 0.9757\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1519 - accuracy: 0.9668 - val_loss: 0.1567 - val_accuracy: 0.9807\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1255 - accuracy: 0.9826 - val_loss: 0.1417 - val_accuracy: 0.9838\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1187 - accuracy: 0.9821 - val_loss: 0.1302 - val_accuracy: 0.9838\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0990 - accuracy: 0.9908 - val_loss: 0.1206 - val_accuracy: 0.9848\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1069 - accuracy: 0.9823 - val_loss: 0.1123 - val_accuracy: 0.9868\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0935 - accuracy: 0.9842 - val_loss: 0.1055 - val_accuracy: 0.9888\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0834 - accuracy: 0.9914 - val_loss: 0.0996 - val_accuracy: 0.9899\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0715 - accuracy: 0.9937 - val_loss: 0.0940 - val_accuracy: 0.9899\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0681 - accuracy: 0.9981 - val_loss: 0.0895 - val_accuracy: 0.9899\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0676 - accuracy: 0.9973 - val_loss: 0.0855 - val_accuracy: 0.9899\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0673 - accuracy: 0.9988 - val_loss: 0.0815 - val_accuracy: 0.9899\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0564 - accuracy: 0.9931 - val_loss: 0.0783 - val_accuracy: 0.9899\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0505 - accuracy: 0.9950 - val_loss: 0.0755 - val_accuracy: 0.9888\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0584 - accuracy: 0.9950 - val_loss: 0.0730 - val_accuracy: 0.9888\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0513 - accuracy: 0.9988 - val_loss: 0.0705 - val_accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what’s the final verdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 0.9840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12341559678316116, 0.984000027179718]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06529685109853745, 0.9944999814033508]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this model’s test accuracy is 99.45%, which means that transfer learning reduced the error rate from 1.6% down to almost 0.55%! That’s a factor of 2.9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9090909090909136"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 98.4) / (100 - 99.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`Are you convinced? You shouldn’t be: I cheated! I tried many configurations until I found one that demonstrated a strong improvement. If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses. What I did is called “torturing the data until it confesses.” When a paper just looks too positive, you should be suspicious: perhaps the flashy new technique does not actually help much (in fact, it may even degrade performance), but the authors tried many variants and reported only the best results (which may be due to sheer luck), without mentioning how many failures they encountered on the way. Most of the time, this is not malicious at all, but it is part of the reason so many results in science can never be reproduced.`*** <br>\n",
    "\n",
    "***`Why did I cheat? It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers).`*** `We will revisit transfer learning in Chapter 14, using the techniques we just discussed (and this time there will be no cheating, I promise!).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to tackle a complex task for which you don’t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don’t lose hope! **First, you should try to gather more labeled training data, but if you can’t, you may still be able to perform `unsupervised pretraining`** (see Figure 11-5). Indeed, it is often cheap to gather unlabeled training examples, but expensive to label them. If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network (see Chapter 17). *Then you can reuse the lower layers of the autoencoder or the lower layers of the GAN’s discriminator*, add the output layer for your task on top, and fine-tune the final network using supervised learning (i.e., with the labeled training examples).\n",
    "\n",
    "It is this technique that Geoffrey Hinton and his team used in 2006 and which led to the revival of neural networks and the success of Deep Learning. Until 2010, unsupervised pretraining—typically with restricted Boltzmann machines (RBMs; see Appendix E)—was the norm for deep nets, and only after the vanishing gradients problem was alleviated did it become much more common to train DNNs purely using supervised learning. Unsupervised pretraining (today typically using autoencoders or GANs rather than RBMs) is still a good option when you have a complex task to solve, no similar model you can reuse, and little labeled training data but plenty of unlabeled training data.\n",
    "\n",
    "Note that in the early days of Deep Learning it was difficult to train deep models, so people would use a technique called **greedy layer-wise pretraining** (depicted in Figure 11-5). They would first train an unsupervised model with a single layer, typically an RBM, then they would freeze that layer and add another one on top of it, then train the model again (effectively just training the new layer), then freeze the new layer and add another layer on top of it, train the model again, and so on. Nowadays, things are much simpler: people generally train the full unsupervised model in one shot (i.e., in Figure 11-5, just start directly at step three) and use autoencoders or GANs rather than RBMs.\n",
    "\n",
    "![In unsupervised training, a model is trained on the unlabeled data (or on all the data) using an unsupervised learning technique, then it is fine-tuned for the final task on the labeled data using a supervised learning technique; the unsupervised part may train one layer at a time as shown here, or it may train the full model directly](images/training_DNN/unsupervised_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network’s lower layers will learn feature detectors that will likely be reusable by the second neural network.\n",
    "\n",
    "For example, if you want to build a system to recognize faces, you may only have a few pictures of each individual—clearly not enough to train a good classifier. Gathering hundreds of pictures of each person would not be practical. You could, however, gather a lot of pictures of random people on the web and train a first neural network to detect whether or not two different pictures feature the same person. Such a network would learn good feature detectors for faces, so reusing its lower layers would allow you to train a good face classifier that uses little training data\n",
    "\n",
    "For natural language processing (NLP) applications, you can download a corpus of millions of text documents and automatically generate labeled data from it. For example, you could randomly mask out some words and train a model to predict what the missing words are (e.g., it should predict that the missing word in the sentence “What ___ you saying?” is probably “are” or “were”). If you can train a model to reach good performance on this task, then it will already know quite a lot about language, and you can certainly reuse it for your actual task and fine-tune it on your labeled data (we will discuss more pretraining tasks in Chapter 15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE**\n",
    "\n",
    "> **`Self-supervised learning`** is when you automatically generate the labels from the data itself, then you train a model on the resulting \"labelled\" dataset using supervised learning techniques. Since this approach requires no human labelling whatsoever, it is best classified as a form of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): **applying a good initialization strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning).** Another huge **speed boost** comes from using a faster optimizer than the regular Gradient Descent optimizer. In this section we will present the most popular algorithms: `momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind momentum optimization, [proposed by Boris Polyak in 1964](https://homl.info/54). In contrast, regular Gradient Descent will simply take small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.\n",
    "\n",
    "Recall that Gradient Descent updates the weights $\\theta$ by directly subtracting the gradient of the cost function $J(\\theta)$ with regard to the weights $(\\nabla_\\theta J(\\theta))$ multiplied by the learning rate $\\eta$. The equation is: $\\theta ← \\theta - \\eta \\nabla_\\theta J(\\theta)$. It does not care about what the earlier gradients were. *If the local gradient is tiny, it goes very slowly.*\n",
    "\n",
    "**Momentum optimization cares a great deal about what previous gradients were**: at each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate $\\eta$), and it updates the weights by adding this momentum vector (see Equation 11-4). `In other words, the gradient is used for acceleration, not for speed.` To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, **`called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Equation 11-4. Momentum algorithm\n",
    "> 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $m ← \\beta m - \\eta \\nabla_\\theta J(\\theta)$\n",
    "> 2. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta ← \\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate $\\eta$ multiplied by $\\frac{1}{1 - \\beta}$ (ignoring the sign). For example, if $\\beta = 0.9$, then the terminal velocity is equal to $10$ times the gradient times the learning rate, so momentum optimization ends up going $10$ times faster than Gradient Descent! This allows momentum optimization to escape from plateaus much faster than Gradient Descent. We saw in Chapter 4 that when the inputs have very different scales, the cost function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes down the steep slope quite fast, but then it takes a very long time to go down the valley. In contrast, `momentum optimization will roll down the valley faster and faster until it reaches the bottom (the optimum).` **`In deep neural networks that don’t use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using momentum optimization helps a lot. It can also help roll past local optima.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "> Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. **This is one of the reasons it’s good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing momentum optimization in Keras is a no-brainer: just use the `SGD` optimizer and set its momentum hyperparameter, then lie back and profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7f9da0600dd8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one drawback of momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nestrov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One small variant to momentum optimization, [proposed by Yurii Nesterov in 1983](https://homl.info/55), is ***almost always faster than vanilla momentum optimization***. The `Nesterov Accelerated Gradient (NAG)` method, also known as `Nesterov momentum optimization`, measures the gradient of the cost function not at the local position $\\theta$ but slightly ahead in the direction of the momentum, at $\\theta + \\beta m$ (see Equation 11-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Equation 11-5. Nesterov Accelerated Gradient algorithm\n",
    "> 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $m ← \\beta m - \\eta \\nabla_\\theta J(\\theta + \\beta m)$\n",
    "> 2. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta ← \\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position, as you can see in Figure 11-6 (where $\\nabla_1$ represents the gradient of the cost function measured at the starting point $\\theta$, and $\\nabla_2$ represents the gradient at the point located at $\\theta + \\beta m$).\n",
    "\n",
    "As you can see, the ***Nesterov update ends up slightly closer to the optimum.*** After a while, these small improvements add up and NAG ends up being significantly faster than regular momentum optimization. **`Moreover, note that when the momentum pushes the weights across a valley,` $\\nabla_1$ `continues to push farther across the valley,` `while` $\\nabla_2$ `pushes back toward the bottom of the valley. This helps reduce oscillations and thus NAG converges faster.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAG is generally faster than regular momentum optimization. To use it, simply set $nesterov=True$ when creating the SGD optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7f9d169b7080>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regular versus Nesterov momentum optimization: the former applies the gradients computed before the momentum step, while the latter applies the gradients computed after](images/training_DNN/Regular_versus_Nesterov_momentum_optimization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The [AdaGrad algorithm](https://homl.info/56) achieves this correction by scaling down the gradient vector along the steepest dimensions (see Equation 11-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Equation 11-6. AdaGrad algorithm\n",
    "> 1. &emsp; $s ← s + \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$\n",
    "> 2. &emsp; $\\theta ← \\theta - \\eta \\nabla_\\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step accumulates the square of the gradients into the vector $s$ (recall that the $\\otimes$ symbol represents the element-wise multiplication). This vectorized form is equivalent to computing $s_i$ &#8592; $s_i$ $+$ $(\\partial J(\\theta)$ $/$ $\\partial \\theta_i)^2$ for each element $s_i$ of the vector $\\mathbf s$; in other words, each $s_i$ accumulates the squares of the partial derivative of the cost function with regard to parameter $θ_i$. If the cost function is steep along the $i^{th}$ dimension, then $s_i$ will get larger and larger at each iteration.\n",
    "\n",
    "The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor of $\\sqrt{s + \\epsilon}$ (the $\\oslash$ symbol represents the element-wise division, and $\\epsilon$ is a smoothing term to avoid division by zero, typically set to $10^{–10}$). This vectorized form is equivalent to simultaneously computing $\\theta_i$ &#8592; $\\theta_i$ $-$ $\\eta \\partial J(\\theta)$ $/$ $\\partial \\theta_i$ $/$ $\\sqrt{s_i + \\epsilon}$ for all parameters $\\theta_i$.\n",
    "\n",
    "**In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. `This is called an adaptive learning rate.` It helps point the resulting updates more directly toward the global optimum (see Figure 11-7). One additional benefit is that it requires much less tuning of the learning rate hyperparameter $\\eta$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AdaGrad versus Gradient Descent: the former can correct its direction earlier to point to the optimum](images/training_DNN/AdaGrad_versus_Gradient_Descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaGrad frequently performs well for simple quadratic problems, `but it often stops too early when training neural networks.` The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an Adagrad optimizer, `you should not use it to train deep neural networks` (it may be efficient for simpler tasks such as Linear Regression, though). Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate optimizers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad at 0x7f9d15cd7978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. The RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step (see Equation 11-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Equation 11-7. RMSProp algorithm\n",
    "> 1. &emsp; $s ← \\beta s + (1- \\beta) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$\n",
    "> 2. &emsp; $\\theta ← \\theta - \\eta \\nabla_\\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decay rate $\\beta$ is typically set to $0.9$. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all.\n",
    "\n",
    "As you might expect, Keras has an *RMSProp* optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop at 0x7f9d1662efd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, `it was the preferred optimization algorithm of many researchers until Adam optimization came around.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Adam](https://homl.info/59), which stands for **`adaptive moment estimation`, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients** (see Equation 11-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Equation 11-8. Adam algorithm\n",
    "> 1. &emsp; $\\mathbf m ← \\beta_1 \\mathbf m - (1 - \\beta_1)\\nabla_\\theta J(\\theta)$\n",
    "> 2. &emsp; $\\mathbf {s} ← \\beta_2 \\mathbf s + (1- \\beta_2) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)$\n",
    "> 3. &emsp; $\\mathbf {\\hat m} ← \\frac {\\mathbf m}{1 - \\beta_1^T}$\n",
    "> 4. &emsp; $\\mathbf {\\hat s} ← \\frac {\\mathbf {s}}{1 - \\beta_2^T}$\n",
    "> 5. &emsp; $\\theta ← \\theta + \\eta \\mathbf {\\hat m} \\oslash \\sqrt{\\mathbf {\\hat s} + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation, $t$ represents the iteration number (starting at 1).\n",
    "\n",
    "If you just look at steps 1, 2, and 5, you will notice Adam's close similarity to both momentum optimization and RMSProp. the only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just $1 - \\beta_1$ times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since $\\mathbf m$ and $\\mathbf s$ are initialized at $0$, they will be biased toward $0$ at the beginning of training, so these two steps will help boost $\\mathbf m$ and $\\mathbf s$ at the beginning of training.\n",
    "\n",
    "The momentum decay hyperparameter $\\beta_1$ is typically initialized to $0.9$, while the scaling decay hyperparameter $\\beta_2$ is often initialized to $0.999$. As earlier, the smoothing term $\\epsilon$ is usually initialized to a tiny number such as $10^{–7}$. These are the default values for the Adam class (to be precise, epsilon defaults to None, which tells Keras to use `keras.backend.epsilon()`, which defaults to $10^{–7}$; you can change it using `keras.backend.set_epsilon()`). Here is how to create an Adam optimizer using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7f9d16497160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter $\\eta$. You can often use the default value $\\eta = 0.001$, making Adam even easier to use than Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIP\n",
    "\n",
    "> If you are starting to feel overwhelmed by all these different techniques and are wondering how to choose the right ones for your task, don't worry: some practical guidelines are provided at the end of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two variants of Adam are worth mentioning:\n",
    "\n",
    "***`AdaMax`***\n",
    "\n",
    "> Notice that in step 2 of Equation 11-8, Adam accumulates the squares of the gradients in $\\mathbf s$ (with a greater weight for more recent weights). In step 5, if we ignore $\\epsilon$ and steps 3 and 4 (which are technical details anyway), Adam scales down the parameter updates by the square root of $\\mathbf s$. In short, Adam scales down the parameter updates by the $l_2$ norm of the time-decayed gradients (recall that the $l_2$ norm is the square root of the sum of squares). AdaMax, introduced in the same paper as Adam, replaces the $l_2$ norm with the $l_∞$ norm (a fancy way of saying the max). Specifically, it replaces step 2 in Equation 11-8 with $\\mathbf s ← $ max $(\\beta_2 s, \\nabla_\\theta J(\\theta))$, it drops step 4, and in step 5 it scales down the gradient updates by a factor of $\\mathbf s$, which is just the max of the time-decayed gradients. **`In practice, this can make AdaMax more stable than Adam, but it really depends on the dataset, and in general Adam performs better. So, this is just one more optimizer you can try if you experience problems with Adam on some task.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adamax.Adamax at 0x7f9d1649eeb8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Nadam`**\n",
    "\n",
    "> Nadam optimization is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. In [his report introducing this technique](https://homl.info/nadam), the researcher Timothy Dozat compares many different optimizers on various tasks and ***`finds that Nadam generally outperforms Adam but is sometimes outperformed by RMSProp.`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.nadam.Nadam at 0x7f9d166c7240>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WARNING**\n",
    "\n",
    "> ***`Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, converging fast to a good solution. However, a` [2017 paper](https://homl.info/60) `by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also check out the latest research, because it’s moving fast.`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the optimization techniques discussed so far only rely on the _first-order partial derivatives (Jacobians)_. The optimization literature also contains amazing algorithms based on the _second-order partial derivatives_ (the _Hessians_, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are n2 Hessians per output (where n is the number of parameters), as opposed to just n Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRAINING SPARSE MODELS**\n",
    "\n",
    "> **`All the optimization algorithms just presented produce dense models, meaning that most parameters will be nonzero. If you need a blazingly fast model at runtime, or if you need it to take up less memory, you may prefer to end up with a sparse model instead.` <br> <br>\n",
    "> One easy way to achieve this is to train the model as usual, then get rid of the tiny weights (set them to zero). Note that this will typically not lead to a very sparse model, and it may degrade the model’s performance. <br> <br>\n",
    "> A better option is to apply strong $l_1$ regularization during training (we will see how later in this chapter), as it pushes the optimizer to zero out as many weights as it can (as discussed in “Lasso Regression” in Chapter 4). <br> <br>\n",
    "> If these techniques remain insufficient, check out the [TensorFlow Model Optimization Toolkit (TF-MOT)](https://homl.info/tfmot), which provides a pruning API capable of iteratively removing connections during training based on their magnitude.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 11-2 compares all the optimizers we’ve discussed so far (* is bad, ** is average, and *** is good).\n",
    "\n",
    "![Optimizer comparison](images/training_DNN/optimizer_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Learning Rate Scheduling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a good learning rate is very important. If you set it much too high, training may diverge (as we discussed in “Gradient Descent”). If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Learning curves for various learning rates $\\eta$](images/training_DNN/learning_curves_for_various_learning_rates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in Chapter 10, you can find a good learning rate by training the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value, and then looking at the learning curve and picking a learning rate slightly lower than the one at which the learning curve starts shooting back up. You can then reinitialize your model and train it with that learning rate.\n",
    "\n",
    "But you can do better than a constant learning rate: ***if you start with a large learning rate and then reduce it once training stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. It can also be beneficial to start with a low learning rate, increase it, then drop it again. These strategies are called `learning schedules`*** (we briefly introduced this concept in Chapter 4). These are the most commonly used learning schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Power scheduling`**\n",
    "\n",
    "> Set the learning rate to a function of the iteration number $t$: $\\eta(t)$ $=$ $\\eta_0$ $/$ $(1$ $+$ $t/s)^c$. The initial learning rate $\\eta_0$, the power $c$ (typically set to $1$), and the steps $s$ are hyperparameters. The learning rate drops at each step. After $s$ steps, it is down to $\\eta_0$ $/$ $2$. After $s$ more steps, it is down to $\\eta_0$ $/$ $3$, then it goes down to $\\eta_0$ $/$ $4$, then $\\eta_0$ $/$ $5$, and so on. As you can see, this schedule first drops quickly, then more and more slowly. Of course, power scheduling requires tuning $\\eta_0$ and $s$ (and possibly $c$).\n",
    "\n",
    "> `lr = lr0 / (1 + steps / s)**c`\n",
    "> * Keras uses `c = 1` and `s = 1 / decay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7f859b428a20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 13s 6ms/step - loss: 0.6012 - accuracy: 0.7886 - val_loss: 0.4093 - val_accuracy: 0.8596\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3852 - accuracy: 0.8648 - val_loss: 0.3735 - val_accuracy: 0.8716\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3463 - accuracy: 0.8763 - val_loss: 0.3551 - val_accuracy: 0.8762\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3256 - accuracy: 0.8852 - val_loss: 0.3528 - val_accuracy: 0.8780\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3061 - accuracy: 0.8930 - val_loss: 0.3464 - val_accuracy: 0.8792\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2972 - accuracy: 0.8927 - val_loss: 0.3423 - val_accuracy: 0.8822\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2866 - accuracy: 0.8991 - val_loss: 0.3315 - val_accuracy: 0.8824\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2785 - accuracy: 0.8994 - val_loss: 0.3363 - val_accuracy: 0.8796\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2681 - accuracy: 0.9050 - val_loss: 0.3252 - val_accuracy: 0.8848\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2561 - accuracy: 0.9084 - val_loss: 0.3315 - val_accuracy: 0.8850\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2572 - accuracy: 0.9092 - val_loss: 0.3275 - val_accuracy: 0.8858\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2447 - accuracy: 0.9134 - val_loss: 0.3267 - val_accuracy: 0.8830\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2454 - accuracy: 0.9126 - val_loss: 0.3216 - val_accuracy: 0.8898\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2343 - accuracy: 0.9171 - val_loss: 0.3209 - val_accuracy: 0.8908\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2355 - accuracy: 0.9175 - val_loss: 0.3165 - val_accuracy: 0.8900\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2313 - accuracy: 0.9194 - val_loss: 0.3218 - val_accuracy: 0.8856\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2256 - accuracy: 0.9208 - val_loss: 0.3160 - val_accuracy: 0.8896\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2213 - accuracy: 0.9222 - val_loss: 0.3183 - val_accuracy: 0.8890\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2179 - accuracy: 0.9231 - val_loss: 0.3210 - val_accuracy: 0.8852\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2135 - accuracy: 0.9259 - val_loss: 0.3169 - val_accuracy: 0.8902\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2126 - accuracy: 0.9263 - val_loss: 0.3195 - val_accuracy: 0.8900\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2144 - accuracy: 0.9256 - val_loss: 0.3183 - val_accuracy: 0.8880\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2114 - accuracy: 0.9269 - val_loss: 0.3159 - val_accuracy: 0.8892\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2050 - accuracy: 0.9294 - val_loss: 0.3190 - val_accuracy: 0.8896\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2008 - accuracy: 0.9296 - val_loss: 0.3151 - val_accuracy: 0.8874\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = math.ceil(len(X_train_scaled) / batch_size)\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs, 'o-')\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Exponential scheduling`**\n",
    "\n",
    "> Set the learning rate to $\\eta(t)$ $=$ $\\eta_0$ $0.1^{t/s}$. The learning rate will gradually drop by a factor of 10 every s steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every s steps.\n",
    "\n",
    "> `lr = lr0 * 0.1**(epoch / s)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Nadam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 1.7034 - accuracy: 0.4071 - val_loss: 1.3224 - val_accuracy: 0.4710\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.008912509381337455.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.3026 - accuracy: 0.4751 - val_loss: 1.2261 - val_accuracy: 0.5258\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.007943282347242816.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.2129 - accuracy: 0.5165 - val_loss: 1.1948 - val_accuracy: 0.5520\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0070794578438413795.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.1289 - accuracy: 0.5399 - val_loss: 1.1393 - val_accuracy: 0.5606\n",
      "Epoch 5/25\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006309573444801933.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.0787 - accuracy: 0.5552 - val_loss: 1.0912 - val_accuracy: 0.5754\n",
      "Epoch 6/25\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.005623413251903491.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.0537 - accuracy: 0.5616 - val_loss: 1.1059 - val_accuracy: 0.5824\n",
      "Epoch 7/25\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.005011872336272724.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 1.0109 - accuracy: 0.5655 - val_loss: 1.1301 - val_accuracy: 0.5752\n",
      "Epoch 8/25\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.004466835921509631.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.9907 - accuracy: 0.5684 - val_loss: 1.1342 - val_accuracy: 0.5780\n",
      "Epoch 9/25\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0039810717055349725.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.9936 - accuracy: 0.5676 - val_loss: 1.0855 - val_accuracy: 0.5810\n",
      "Epoch 10/25\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.003548133892335755.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.9645 - accuracy: 0.5727 - val_loss: 1.0448 - val_accuracy: 0.5722\n",
      "Epoch 11/25\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0031622776601683794.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.9493 - accuracy: 0.5740 - val_loss: 1.0530 - val_accuracy: 0.5788\n",
      "Epoch 12/25\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.002818382931264454.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.9344 - accuracy: 0.5776 - val_loss: 1.0079 - val_accuracy: 0.5906\n",
      "Epoch 13/25\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0025118864315095803.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.9237 - accuracy: 0.5787 - val_loss: 1.0141 - val_accuracy: 0.5858\n",
      "Epoch 14/25\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0022387211385683395.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.9188 - accuracy: 0.5810 - val_loss: 1.0180 - val_accuracy: 0.5896\n",
      "Epoch 15/25\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0019952623149688802.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.9044 - accuracy: 0.5818 - val_loss: 1.0663 - val_accuracy: 0.5880\n",
      "Epoch 16/25\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.001778279410038923.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.9009 - accuracy: 0.5841 - val_loss: 1.0728 - val_accuracy: 0.5864\n",
      "Epoch 17/25\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0015848931924611134.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.8936 - accuracy: 0.5849 - val_loss: 0.9940 - val_accuracy: 0.5902\n",
      "Epoch 18/25\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0014125375446227546.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.8848 - accuracy: 0.5849 - val_loss: 0.9970 - val_accuracy: 0.5864\n",
      "Epoch 19/25\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0012589254117941673.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.8796 - accuracy: 0.5865 - val_loss: 0.9831 - val_accuracy: 0.5888\n",
      "Epoch 20/25\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0011220184543019637.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.8744 - accuracy: 0.5885 - val_loss: 0.9897 - val_accuracy: 0.5894\n",
      "Epoch 21/25\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.8586 - accuracy: 0.5973 - val_loss: 0.9288 - val_accuracy: 0.6488\n",
      "Epoch 22/25\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0008912509381337455.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.6305 - accuracy: 0.7180 - val_loss: 0.7553 - val_accuracy: 0.7382\n",
      "Epoch 23/25\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0007943282347242814.\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.5701 - accuracy: 0.7517 - val_loss: 0.7361 - val_accuracy: 0.7476\n",
      "Epoch 24/25\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0007079457843841381.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.5556 - accuracy: 0.7579 - val_loss: 0.7265 - val_accuracy: 0.7580\n",
      "Epoch 25/25\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0006309573444801933.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5455 - accuracy: 0.7624 - val_loss: 0.7434 - val_accuracy: 0.7544\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn, verbose=1)\n",
    "history = model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), epochs=n_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEeCAYAAAC30gOQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA240lEQVR4nO3deXxU1f3/8dcnGwkECAEECbIpYkE2EdeiqFVs1a+4dNO6tHVt/WnrbtXWaq2itbVWq1K3urVqi6KiohajuKCyKKuAgghh3wKBQFg+vz/uDQ7DTDLBzEySeT8fj3kkc86ZO5+5hHxy7jn3HHN3RERE6ltWugMQEZGmSQlGRESSQglGRESSQglGRESSQglGRESSQglGRESSQglGJAXM7Fwzq6jja0rN7N5kxRS+x5dmdmUSjnu6mdXpHojoc7Q750waFiUYSSoze8zMPMZjQrpjS5bw850eVfwM0CMJ73WemU0xswozKzezqWb2h/p+nzRJyjmT1MlJdwCSEd4Ezooqq0pHIOni7pVAZX0e08x+BtwD/Br4H5AL7A8cWp/vky7JOGeSWurBSCpsdvelUY/VAGZ2pJltMbOh1Y3N7EIzW2dmPcLnpWb2gJn91czWhI87zSwr4jVtzOyfYV2lmb1pZn0i6s8N/8o/xsymm9kGM3vLzLpHBmpmJ5nZJDPbZGbzzexWM8uLqP/SzG4wswfDGBeZ2VWR9eG3z4U9mS8j3z+i3d5mNtrMloaxTDazE+t4Xv8PGOXuD7r75+4+y92fc/fLoz7T98zsw/C8rDKzl8wsP6JJfrzPE76+tZmNNLPlZrbezN42swOj2pxtZgvMbKOZvQx0iKq/ycymR5XVeAksxjm7Kfy3+5GZfRHG8oKZtYtok2Nmf4n4OfmLmd1vZqW1n06pb0owklbu/jZwJ/BEmCT2A/4M/D93nxfR9EyCn9dDgQuBC4BfRdQ/BhwMnAwcBGwEXjOzgog2zYDrgJ+FxykCHqiuNLNhwFPAvUCfsN3pwB+jwv41MA04ABgB3GFm1b2GweHX84E9I55HKwReBY4F+gP/BUaFnz9RS4GDqhNxLGZ2PPAi8AYwCDgKeJud/+/H/TxmZsAYoAQ4ERgIvAOMM7M9wzYHE5z/kcAA4CXg5jp8jrroBvwQOAU4Lozn1oj6K4FzgfOAQwg+5xlJikVq4+566JG0B8Evnq1ARdRjRESbXOBjYBQwGXgm6hilwBzAIspuABaF3/cEHDgior41UA6cFz4/N2zTK6LNmcDm6uMS/OK8Meq9h4fxVrf5EvhXVJu5wA0Rzx04ParNuUBFLedqQtRxSoF7a2i/J/BB+H5zgSeBs4HciDbvAf+u4Rg1fh7g6PDzF0S1+QS4Ovz+aeCNqPqHgl8vO57fBEyv6Zwk8PwmYBPQOqLseuDziOdLgGsjnhswGyhN9/+FTHyoByOp8A7BX7aRjzurK919C8FfmScCexD0UKJN8PA3RugDoMTMWgHfAraHZdXHLCf4q7x3xGs2u/vsiOeLgTygTfh8EHB9eCmtIrw88zTQAugY8bqpUbEtDuNOmJm1MLM7zGxmeCmnAjgQ6JLoMdx9ibsfCvQF7ib4Zfog8JGZNQ+bDSQYn6lJTZ9nENAcWBF1XvYH9g7bfIuIcx+Kfl5fFoT/trvEamatCf6dPqquDH9mPkLSQoP8kgob3f3zWtpUX84oAtoDa+vpvSOT0tY4dVkRX38PPBfjOCsivt8S4zh1/WPtT8DxBJd05hJc0nucIOHVibtPB6YD95nZt4HxwA8Ieo+JqOnzZAHLgCExXreuDmFuJ0iAkXLr8Ppq9XHuJUX0DyNpFw603wv8kmCs4Ekzi/7j5+BwPKDaIcBid18HzOLr8ZnqY7Yi+Mt+Zh1CmQzs58GAefQjOjnVZAuQXUubbwOPu/t/3X0qsIivewTfRPXnLQy/TgGO+QbHm0wwYL89xjlZHraZRfDvESn6+QqgQ9S/4YBvENcuwp7NUiLGvcL3izcOJkmmHoykQjMz6xhVts3dV5hZNvAE8La7P2hm/yG4tPU74MaI9p2Au83s7wSJ4yrgDwDuPtfMRgMPmtkFBL2fWwn+wn66DnHeDLxsZguAZwl6PPsDB7n71XU4zpfAMWb2NsFluTUx2swBTgnj3kLwefNjtIvLzO4nuEQ0jiBB7UkwNrUReD1sdivwkpl9TnAujGBw/EF335jA27xJMI4z2syuBj4juAx1PPCmu48nmCr9vpldB/wHGEowCB+pFCgGfmNm/w7bRN8rVB/+ClxtZnMIku2FBOdlSRLeS2qhHoykwncI/oNHPqaEdb8B9gF+DuDuq4BzgGvDyz3VniLoFXwI/AN4GPhLRP1PCa61vxh+bQ4c78G9FAlx97HACQQzrT4KH9cCXyX+UQG4IjzGQr7+nNEuB5YTXM56lWCAf3wd3+cNgplzzxIkrOfD8mPdfQ6Au79C8Mv+u2Esb4exbU/kDcIxjO8RJLF/EAyYPwv0IkhuuPsEgn+/iwnGc04lGJCPPM6ssP6CsM2x7Do7rz78ieAPlkcJzikE52VTEt5LalE9M0akwQrvYZju7pekOxZpfMxsCvCuu/+/dMeSaXSJTESaDDPrCgwj6KnlEtyP1C/8KimmBCMiTcl2gnuB7iQYApgJfNfdJ6Y1qgylS2QiIpIUGuQXEZGk0CWyUFFRke+zzz7pDqPB2bBhAy1atEh3GA2OzsuudE5ia+rnZdKkSSvdvX2sOiWYUIcOHZg4UZdpo5WWljJ06NB0h9Hg6LzsSucktqZ+XsL7xmLSJTIREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUmKlCYYMys2s+fNbIOZLTCzM+K0MzMbYWarwscIM7OI+pFmNtvMtpvZuTFe/2szW2pm68zsETNrVltsX67bzuG3j+OFKWXf6DOKiEgg1T2Y+4AqoANwJnC/mfWJ0e4CYDjQH+gHnARcGFH/KfALYHL0C81sGHAtcAzQFegB/D6R4MrWVnLdqGlKMiIi9SBlCcbMWgCnATe6e4W7vwu8CJwVo/k5wF3uvsjdy4C7gHOrK939Pnf/H7ApzmsfdvcZ7r4GuCXytbWp3LKNO8fOTrS5iIjEkcotk/cFtrr7nIiyT4EjY7TtE9ZFtovV04mlDzA66rUdzKytu6+KbGhmFxD0lsjruM+O8rK1lZSWlib4dk1bRUWFzkUMOi+70jmJLZPPSyoTTCGwLqqsHGgZp215VLtCMzN39wTeJ/q1hO+zU4Jx95HASIBme/bccdySooImvYd2XTT1/cR3l87LrnROYsvk85LKMZgKoFVUWStgfQJtWwEVCSSXeK8lzvvsItuMq4b1SqSpiIjUIJUJZg6QY2Y9I8r6AzNitJ0R1tXWLpZYr10WfXkslpbNctjmTuvmuQm+lYiIxJOyBOPuG4BRwM1m1sLMDgdOBp6I0fxx4HIzKzGzTsAVwGPVlWaWZ2b5gAG5ZpZvZlkRr/25mfU2syLghsjXxtOtVRYTb/wO++xRyPWjplGxeetuf1YREUn9NOVfAAXAcuBfwMXuPsPMhphZRUS7B4GXgGnAdGBMWFbtdaASOIxgDKUSOALA3V8D7gDeAr4CFgC/SyS4ZjnZjDitH0vWbeKO1z7b7Q8pIiKpHeTH3VcT3N8SXT6eYHC++rkDV4ePWMcZWsv7/Bn48+7EOKhrG356WHceeW8+J/brxEHdi3fnMCIiGU9LxcRw5bB96dymgGv+O5VNW7alOxwRkUZJCSaG5nk53H5qP+av3MBf/zc33eGIiDRKSjBxfLtnO35wYGdGvjOP6WXltb9ARER2ogRTg+u/15viFnlc/Z+pbNm2Pd3hiIg0KkowNWjdPJdbTt6fmUvWMfKdeekOR0SkUVGCqcXx+3fke3078tf/zeXz5RW1v0BERAAlmITc9H99KMjN5tr/TmX79kRWqxERESWYBOzRMp8bT+zNxAVreGLCgnSHIyLSKCjBJOi0A0oY0rMdI177jEVrNqY7HBGRBk8JJkFmxh9P6QvAb56fTmILO4uIZC4lmDrYq7g51xy/H+/MWcGoydpWWUSkJkowdXTWIV05sGsbbn55JivWb053OCIiDZYSTB1lZRm3n9aPik1bOOKOcXS/dgyH3z6OF6aoRyMiEimlqyk3FdPLyjEzKrcEd/eXra3kulHTABg+sCSdoYmINBjqweyGO8fOZmvU/TCVW7Zx59jZaYpIRKThUYLZDYvXVtapXEQkEynB7IZORQV1KhcRyURKMLvhqmG9KMjN3qX8R4P3SkM0IiINkxLMbhg+sITbTu1LSVEBBuzZOp/WBTn8d/IiKjZvTXd4IiINgmaR7abhA0t2mjE2Yd4qzvjHBH47ejp//sGA9AUmItJAqAdTTw7p0ZZLju7JqMllPD9lUbrDERFJOyWYenTp0fswuFsbbnh+Ol+u3JDucERE0koJph7lZGdx948GkpOdxaX/nkLVVm2zLCKZSwmmnpUUFTDitL5MXVTOXa/rxksRyVxKMElw/P57cubBXXjwnXm8PWdFusMREUkLJZgkufHE3uzboZArnv1Eqy6LSEZSgkmS/Nxs/vbjA1i/aSuXP/sJ27drgzIRySxKMEnUq2NLbjyxN+PnruShd+elOxwRkZRSgkmyMw/uwvF9OnLHa7P5dOHadIcjIpIySjBJZmbcflpf9mjZjEv/PYX1m7akOyQRkZRIaYIxs2Ize97MNpjZAjM7I047M7MRZrYqfIwwM4uoH2Bmk8xsY/h1QERdMzN7wMyWmdlqM3vJzNK6C1hR8zz++uOBLFy9kd+OnpHOUEREUibVPZj7gCqgA3AmcL+Z9YnR7gJgONAf6AecBFwIYGZ5wGjgSaAN8E9gdFgOcBlwaPi6TsAa4G/J+TiJG9ytmMuO2Zfnp5Qx8ObXtdWyiDR5KUswZtYCOA240d0r3P1d4EXgrBjNzwHucvdF7l4G3AWcG9YNJVik82533+zu9wAGHB3WdwfGuvsyd98EPAPESmIpt1ebArIM1mzcgvP1VstKMiLSFKVyNeV9ga3uPiei7FPgyBht+4R1ke36RNRNdffIeb9Tw/LXgIeBv5pZJ2AtQU/p1VgBmdkFBL0l2rdvT2lpad0+UR3dWrqR6NnKlVu2ccvoTykqn5vU995dFRUVST8vjZHOy650TmLL5POSygRTCKyLKisHWsZpWx7VrjAch4muiz7OXGAhUAZsA6YBl8QKyN1HAiMBevXq5UOHDk3wo+ye1a+NiV2+yUn2e++u0tLSBhtbOum87ErnJLZMPi+pHIOpAFpFlbUC1ifQthVQEfZaajvOfUAzoC3QAhhFnB5MqmmrZRHJJAknGDP7rpm9bGYzzWyvsOw8MzsmwUPMAXLMrGdEWX8g1rSqGWFdrHYzgH6Rs8oIBvSr6wcAj7n7anffTDDAf5CZtUswzqSJt9Xyd/fvmIZoRESSK6EEY2ZnAs8SXH7qDuSGVdnA1Ykcw903EPQmbjazFmZ2OHAy8ESM5o8Dl5tZSTiWcgXwWFhXSnDp69JwSnL15a9x4dePgbPNrLWZ5QK/ABa7+8pE4kym6K2WO7XOp3NRAf/+eCFzl8XqyImINF6J9mCuBs53918DkZvOTyDoMSTqF0ABsBz4F3Cxu88wsyFmVhHR7kHgJYLxk+nAmLAMd68imMJ8NsEg/s+A4WE5wJXAJoJkuAL4HnBKHWJMquEDS3jv2qOZf/sJvH/dMTx38aHk52Zz/uMTKd+omzBFpOlIdJC/J/BBjPJY4yFxuftqguQQXT6eYPC++rkTJLWYvSN3nwIMilO3imDmWKOwZ+sCHjzrAH488kMu+ddkHj13MDnZWmBBRBq/RH+TLSaYZhztCOCL+gsnMw3qWswfhu/P+Lkr+eMrn6U7HBGRepFoghkJ3BOOmwDsZWbnAHcA9yclsgzzg8F7ce5h3Xjkvfk8N3FhusMREfnGErpE5u53mFlr4A0gH3gL2Az8yd3vS2J8GeWGE77F3OXruf756fRoX8igrm3SHZKIyG5L+GK/u18PtAMOAg4B2rv7jckKLBPlZGdx748PYM+ifC56chJLyzelOyQRkd2W6DTlR8yspbtvdPeJ7v6Ru1eE040fSXaQmaRNizz+cfaBbNy8lQuemMimLdvSHZKIyG5JtAdzDsH04mgFBNOFpR7t26Eld/9oINPKyrn2v1PZedk1EZHGocYEE+7f0pZgteI24fPqR3vgRGBZKgLNNMf27sAVx+7LC58sZuQ72m5ZRBqf2gb5VwIePmbGqHfgd/UdlAR+edQ+zFq6nttf+4x9O7bkqF57pDskEZGE1ZZgjiLovYwj2MtldURdFbDA3RcnKbaMZ2bceXo/5q/YwEWPT6R18zxWrN9Mp6ICrhrWi+ED07pRp4hIjWpMMO7+NoCZdQcWuvv2lEQlOzTPy+H7B3bm9y/NZPn6zcDXG5UBSjIi0mAleh/MAoBw4ckuQF5U/Tv1H5pUe2j8/F3KKrds486xs5VgRKTBSijBhInlaYKlYZzgslnk1KZd16CXerN4bWWdykVEGoJEpynfTbBEfm9gIzAE+D4wCzg+KZHJDvE3KstPcSQiIolLNMEcCVzj7p8R9FxWuPso4BrglmQFJ4F4G5X161yU+mBERBKUaIIpIJiyDMFMsur5sjMJdpOUJNplo7KifAZ3bcOr05fyxIQF6Q5PRCSmRPeD+QzYD/gS+AS4yMwWAr8EypISmexk+MCSnQb0t2zbzkVPTOK3o6dT3DyPE/rtmcboRER2lWgP5q9A9cbxNwPHAfMIdqj8TRLiklrkZmdx7xkHcGDXNvzqmSm8OzftO0KLiOwkoQTj7k+5+2Ph95OBbsBgoIu7P5e06KRGBXnZPHTOYPZuX8gFT0zk04Vr0x2SiMgOu7U3b7iq8mRgg5ldW88xSR20Lsjl8Z8dRHGLPM599CM+X16R7pBERIAEEoyZtTOzE8zsODPLDstyzexXBGMyVyY3RKnNHq3yefLnB5OdZZz98IcsKdf9MSKSfrWtpnwYMBd4CXgVeM/M9gOmApcQTFHukuwgpXbd2rXgsZ8exPpNWznr4Y9Ys6Eq3SGJSIarrQdzCzCWYCry3QS7Wb4M3Ab0dPd73X1jUiOUhO1f0pqRZx/IV6s38tPHPmZj1dZ0hyQiGay2BNMfuMXdpwM3EtxkeZ27P+7aBatBOnTvtvztxwOZumgtFz05maqtWp9URNKjtvtgioEVEAzsm9lGYErSo5JvZFifjtx2al+u+e80fvTgByxdv4klazdpmX8RSalEbrRsY2Zb+XqBy1ZmVhzZwN1Xx3ylpM0PB3dh/NwVvDx16Y4yLfMvIqmUyDTlmQS9mOVAIfBx+HwFwfIxK5IWnXwjU75au0tZ9TL/IiLJlsiOltJILV67KU65pjGLSPIltKOlNE6digooi5FMtMy/iKTCbt3JL41DvGX+e7QvRJMARSTZUppgzKzYzJ43sw1mtsDMzojTzsxshJmtCh8jzMwi6geY2SQz2xh+HRD1+gPM7B0zqzCzZWZ2WZI/WoMUvcx/SVE+R/Zsx/i5K7n+hels364kIyLJk+hy/fXlPqAK6AAMAMaY2afuPiOq3QXAcIL7cBx4A5gPPGBmecBoghs//w5cCIw2s57uXmVm7YDXgF8D/wHygM7J/VgNV/Qy/+7OHWNnc3/pF2zZup3bT+tHdpbVcAQRkd2Tsh6MmbUATgNudPcKd38XeBE4K0bzc4C73H2Ru5cBdwHnhnVDCRLj3e6+2d3vIZhCfXRYfzkwNlwBerO7r3f3WUn7YI2MmXH1sF5cdkxPnpu0iCue/YSt23QzpojUv1T2YPYFtrr7nIiyTwm2Y47WJ6yLbNcnom5q1EoCU8Py14BDgGlm9j6wD/Ah8Et3/yr6TczsAoLeEu3bt6e0tHQ3PlbjNDAXTuuZy38/WUzZ0mVc2K8ZOTF6MhUVFRl1XhKl87IrnZPYMvm8JJRgzOyROFUObAI+B55x98U1HKYQWBdVVg60jNO2PKpdYTgOE10XfZzOwAHAscA04A7gX8DhuwTvPhIYCdCrVy8fOnRoDeE3PUOHwrfGz+MPY2ZRVNySe88YSLOcnScFlJaWkmnnJRE6L7vSOYktk89Loj2Y9sAQYDswPSzbn+DS1CTgVOBmMxvi7p/EOUYF0CqqrBWwPoG2rYAKd3czq+04lcDz7v4xgJn9HlhpZq3dPToxZbzzhvQgNzuL3704gwufmMQDPxlEfoyZZyIidZXoGMx7BMv1d3b3I9z9CIKewivA60BXYAzBWEk8c4AcM+sZUdYfiB7gJyzrH6fdDKBf5KwygtWeq+unEvSsqmmqVC3OOawbfzylL2/PWcF5/5xIZdW2dIckIk1AognmMuDmyKX5w+9vBX7t7lXACIKZYTG5+wZgFEFPp4WZHQ6cDDwRo/njwOVmVmJmnYArgMfCulJgG3CpmTUzs0vC8nHh10eBU8KpzLkEq0C/q95Lzc44uAt3nt6f979YybmPfsSGzVrqX0S+mUQvkRUCewLRs7E6hnUQjK/UdrxfAI8QrGu2CrjY3WeY2RDgVXevPtaDQA+CMRSAh8IywqnIw8Oy28OYhodJDncfZ2a/IehRNQfeBWLebyM7O31QZ3Kzjcuf/ZQT7hnP5q3bWVK+iZIJ47QKs4jUWaIJ5nngYTO7mmCxS4DBBAPoo8LnBxFcBosrXHV5eIzy8XydqAhniF0dPmIdZwowqIb3uR+4v6ZYJLaTB5TwyVdrePT9BTvKtAqziOyORC+RXUSws+WTwBfh40mCacG/CNvMAs6v7wAl9V6fuXyXMq3CLCJ1lVAPJhxvucjMrgD2Dou/CMdVqtt8Uv/hSTrEW21ZqzCLSF3U6UbLMKFMTVIs0kDEW4W5uEVeGqIRkcYqoUtkZpZvZteY2etm9omZTY18JDtISa1YqzCbwaoNVTzxwZfpCUpEGp1EezB/B04BngPeR/eWNGnVA/l3jp1N2dpKSooKuPTofXh95jJuHD2DhWsqufb4/cjSIpkiUoNEE8xw4Pvu/mYSY5EGpHoV5shlLk4/cC9+/9IMRr4zj4WrN/KXHw7QXf8iEleis8g2AguTGYg0fNlZxu//rw83nPAtXpuxlB//YwKrKjanOywRaaASTTB3ENxZr2siGc7MOG9ID+4/8wBmLl7HKX9/ny9WVKQ7LBFpgBJNMMcCPwS+NLNXzezFyEcS45MG6vj99+TfFxzChs1bOfXv7/PhvFXpDklEGphEE8xKgrv5xwFLCZZ5iXxIBhrYpQ3P/+Jw2hbmcdbDHzH6k7J0hyQiDUiiN1r+NNmBSOPUpW1zRl18GBc+MYnL/v0Jr89YyicL17J47SY6FRVoDTORDJayLZOl6SpqnsfjPz+IQV2KGDNtKWVrN+F8vYbZC1PUsxHJRHF7MOENlEe6+xozm0YN9764e79kBCeNR7OcbJau27RLefUaZurFiGSemi6R/ReonoP6nxTEIo3c4rW7JpigXGuYiWSiuAnG3X8f63uReOKtYdaqIAd3R7PcRTKLxmCk3sRawyzLoLxyK798ejIV2iVTJKMkNIvMzIoJtkc+BtiDqMTk7q3qPzRpbCLXMFu8tpJORQVcedy+LF+/mTvGzuazpe/ywE8GsW+HlmmOVERSIdG1yB4GBgIjgcVosUuJo3oNs2j99yrikqencPK973H7aX05eYAG/UWaukQTzDHAse7+YTKDkabrkB5teeXSb/PLpydz2b8/YfKCNVx/Qm/ycnSVVqSpSvR/93JAC07JN7JHq3yePv8Qzh/SnX9+sIAfPPiBZpiJNGGJJpjrgZvNrDCZwUjTl5udxfUn9Ob+Mw/g8+UVnHDPeMbPXZHusEQkCRK9RHYD0A1YbmYLgC2RlbrRUurqu333pFfHllz85GTOfuQjju/TkU8XrWWJlpgRaTISTTC60VLqXY/2hTz/y8M4++EPeXX60h3l1UvMAEoyIo1YrQnGzHKBFsB97r4g+SFJJmmel8OSci0xI9IU1ToG4+5bgIsB3YYtSaElZkSapkQH+V8Hjk5mIJK5OhUVxCzPyjImLViT4mhEpL4kmmD+B/zRzO42s7PM7NTIRzIDlKYv1hIzeTlZtGyWw/cfeJ87x35G1dbtaYpORHZXooP894ZfL41R50B2jHKRhMRaYuaqYb045lt7cMvLM7nvrS9467MV/OWHA+jVUcvMiDQWie5oqdutJaniLTFzx+n9ObZ3R64bNZWT/vYuVw7bl59/uwfZWRoSFGnolDikwTu2dwfG/uoIjtqvPX985TN+PHICC1dvTHdYIlKLRC+RYWZtgO8CXYC8yDp3vznBYxQTLJx5HLASuM7dn47RzoDbgfPCooeAa93dw/oB4XG+BcwCfu7un0QdIw/4FGjp7p0T+pDSYLUtbMYDPxnEqMll3PTiDI6/+x1uPLE3zXKy+NPrc3a6tKapzSINQ6LL9R8CjCHY4bI9UAbsGT7/EkgowQD3AVVAB2AAMMbMPnX3GVHtLgCGA/0JxnjeAOYDD4SJYzRwN/B34EJgtJn1dPeqiGNcBawAdNG+iTAzThvUmYN7FHPVc1O5dtQ0sgy2h2t76wZNkYYl0UtkdwJPASXAJoIpy12AicCIRA5gZi2A04Ab3b3C3d8FXgTOitH8HOAud1/k7mXAXcC5Yd1QgsR4t7tvdvd7CO7R2TGN2sy6Az8Bbkvw80kj0rlNc54672BaF+TsSC7Vqm/QFJH0S/QSWT+Cy1BuZtuAZu4+z8yuAZ4mSD612RfY6u5zIso+BY6M0bZPWBfZrk9E3dTqy2WhqWH5a+HzvwG/AWq8U8/MLiDoLdG+fXtKS0sT+BiZpaKiosGel/LK2Dtklq2tTHrMDfm8pIvOSWyZfF4STTCRl56WAV0Jxj4qgE4JHqMQWBdVVk7sS1iFYV1ku8JwbCa6bqfjmNkpQLa7P29mQ2sKyN1HEmyiRq9evXzo0BqbZ6TS0lIa6nkpmTCOshh3+7dslsMhhw8hPzd5s+cb8nlJF52T2DL5vCR6iWwyMDj8vhT4g5mdA9xD0HtIRAUQvbVyK2B9Am1bARVhryXuccLLcHcQ+34daWJi3aCZbcb6zVsZdvc7lM5enqbIRATqth/M4vD7GwgGz/8GtCG8xJSAOUCOmfWMKOsPRA/wE5b1j9NuBtAv7M1U6xeW9yTYVmC8mS0FRgF7mtlSM+uWYJzSSAwfWMJtp/alpKgAA0qKCrjrB/156ryDyTbj3Ec/5pdPTWZpjMU0RST5Er3RcmLE9ysIpivXibtvMLNRBBuXnUcwi+xk4LAYzR8HLjezVwhmkV1BkNAg6EFtAy41sweA88PyccB2YK+I4xxGsArBAQRJUZqYeDdovvqrIYx8ex73vvU5pbOXc/lxvTjn0K7kZOvWL5FUqdP/NjM70Mx+GF6KwsxamFnC99IAvwAKCLZg/hdwsbvPMLMhZha5JfODwEvANGA6wRTpBwHCqcjDgbOBtcDPgOHuXuXuW919afUDWA1sD59vq8tnlcatWU42/++Ynrzx6yMZ3L2YW16eyUn3vsfkr7R4pkiqJHofTAeCe08OIuhR9ATmAX8mmLZ8WSLHcffVBMkhunw8weB99XMHrg4fsY4zBRiUwPuVArrJMoN1aducR88dzGvTl/L7l2Zy2v3v86PBXehb0or73vpCN2iKJFGivY+/EMweawt8FVH+HF9fuhJpkMyM7/bdkyH7tufuN+bw8Lvz+VdEvW7QFEmORC+RHQNc7+7R1xe+ILjhUqTBK2yWww0n9qZ9y2a71OkGTZH6l2iCKWDne2GqtSe4RCbSaKxYvzlmuXbQFKlfiSaYd/h6qRYAN7Ns4BqCzchEGo14O2g68Jvnp7F8vf5mEqkPiSaYq4HzzewNoBnB2mAzgcOB65IUm0hSxLpBMz83iyE92/HsxwsZemcpf3ljDhs2x16KRkQSk+h9MDPNrC9wMcEKyvkEA/z3ufuSJMYnUu/i7aA5fGAJX67cwJ2vz+av/5vLUx9+xa++05MfDt6LXN0/I1JnCd/DEt5X8rvIMjPrambPuvsP6j0ykSSKd4Nmt3YtuO+MAzjv22u47ZXPuOGF6Tzy3nyuOX4/juvdgdGfLObOsbMpW1tJyYRxmt4sUoO63CQZSxHBEvwiTcrALm145sJD+N+s5dz+2mdc+MQkurdtzuLyTWzeuh3Q9GaR2qjfLxKHmfGd3h147bIh3HZqXxas3rgjuVTT9GaR+JRgRGqRk53Fjw/qgnvsek1vFolNCUYkQfGmNzfLyeLThWtTG4xII1DjGIyZvVjL66P3ZRFpsq4a1ovrRk2jcsvX66bmZBlmcPJ973HEvu259Oh9OLBbcRqjFGk4ahvkX5VA/fx6ikWkQYuc3ly2tpKScHrzd3p34IkPFvDQ+Hmc/sAHHNKjmEuP7smhe7dl522LRDJLjQnG3X+aqkBEGoPq6c3R2+BePHRvzjmsK//6aCEPvv0FZzz0IYO6tuGSo/dh6L7td0xv1urNkkm+6TRlEQk1z8vh59/uzpkHd+G5iQu5v/QLfvrox+zVpoBl6zZRtS2YJaDpzZIpNMgvUs/yc7M569BulF51FCNO68vi8q+TSzVNb5ZMoAQjkiR5OVn8cHAXtm+PPb9Z05ulqVOCEUmymlZvvuLZT5leVp7agERSRAlGJMlird7cLCeLb+/TllenL+HEv73LDx74gFenLWHrtu1xjiLS+GiQXyTJalq9ubxyC89NXMhj73/JxU9NpqSogHMO68oPD+xC6+a5vDClTLPPpNFSghFJgXirN7cuyOW8IT346eHdeWPmMh59bz5/fOUz/vLGXA7o0pqJC9ZqcU1ptJRgRBqA7Czj+P07cvz+HZmxuJzH3vuS5yYt2qVd9ewzJRhpDDQGI9LA9OnUmju/3594awBo9pk0FkowIg1UTbPPvv/A+zw3cSEbq7StszRcSjAiDVSs2Wf5OVmc1G9PVlVUcdV/pnLQrf/julFTmfLVGjzefgIiaaIxGJEGqqbZZ+7OxAVreObjhbwwZTH/+mgh+3Yo5AcH7sWpB3TmnTkrNPtM0k4JRqQBizf7zMwY3K2Ywd2K+d1JvXl56hKe+Xghfxgziz++MguA6gUENPtM0kWXyEQauZb5ufz4oC688MvDGfurIyjIyyZ6dZrKLdu4Y+xn6QlQMpYSjEgT0qtjSzZu3hazbvHaTdz2yiyml5VrvEZSIqUJxsyKzex5M9tgZgvM7Iw47czMRpjZqvAxwiJ2bjKzAWY2ycw2hl8HRNRdZWbTzWy9mc03s6tS8NFEGox4s8/yc7J4+N35nPi3dzn6rrf58+uz+Xz5+p3avDCljMNvH0f3a8dw+O3jeGFKWSpCliYq1WMw9wFVQAdgADDGzD519xlR7S4AhgP9CWZlvkGwc+YDZpYHjAbuBv4OXAiMNrOe7l4FGHA2MBXYG3jdzBa6+7+T+9FEGoZYWzsX5GZz26l9GdqrPa9NX8pLUxdz71ufc8+4z9mvY0tO6t+J/Jws/vT6nB2v09iNfFMpSzBm1gI4Ddjf3SuAd83sReAs4Nqo5ucAd7n7ovC1dwHnAw8AQ8O47/agn3+PmV0JHA285u53RBxntpmNBg4HlGAkI9Q0+wzgRwd14UcHdWH5+k28MnUJL01dEndvGq0cIN+EpeparJkNBN5z9+YRZVcCR7r7SVFty4Hj3P3D8PmBwFvu3tLMfh3WfTei/cth/V1RxzFgMvCguz8QI6YLCHpLtG/fftCzzz5bT5+26aioqKCwsDDdYTQ4Te28rNi4naveib9CwKPDmhNxlTqmpnZO6ktTPy9HHXXUJHc/MFZdKi+RFQLrosrKgZZx2pZHtSsME0Z0XU3HuYlgnOnRWAG5+0hgJECvXr08co91CUTvPS+Bpnhe7p46jrI4y9D8ZsJ2vvOtDnyndwcO7dGWvJxdh2+b4jmpD5l8XlKZYCqAVlFlrYD1CbRtBVS4u5tZQscxs0sIxmKGuPvmbxK4SCaINXaTn5vFKQNLWL2hiv9MWsQTExZQ2CyHI3u157jeHRi67x68NXs5d46dTdnaSkomjNNNnbJDKhPMHCAnHIyfG5b1B6IH+AnL+gMfxWg3A7jCzMy/vr7Xj2ACAQBm9jOCcZ0jqsdxRKRmtY3dbNqyjfe/WMkbM5fxxszljJm6BAPMdFOnxJayBOPuG8xsFHCzmZ1HMIvsZOCwGM0fBy43s1cId5YF/hbWlQLbgEvN7AGCwX+AcQBmdibwR+Aod5+XnE8j0jTFWzkAID83m6P368DR+3Xg1uHOp4vWctbDH1GxeecFNyu3bOOWl2dybO8OtGimxUIyWar/9X8BPAIsB1YBF7v7DDMbArzq7tUjYQ8CPYBp4fOHwjLcvcrMhodltwOzgOHhFGWAPwBtgY8jBiWfdPeLkvnBRDJJVpYxsEsbNmyOvZrzqg1VDLj5dQZ1bcOQnu05omd7+nRqRVbW1xMFtFtn05fSBOPuqwnub4kuH08weF/93IGrw0es40wBBsWp614fsYpI7ToVFcScGNCuMI/TB+21Y9HNO8fOprhFHt/epx1DerZjQ9VWRrw6W/fcNHHqv4rIbot3U+cNJ/Rm+MASrv3ufqxYv5n3Pl/JO3NXMH7uSl78dHHMY+mem6ZHCUZEdlvkxICytZWUxLjU1b5lsx1jO+7O7GXrOf7u8TGPV7a2kjdnLmNwt2JaN89NyWeQ5FGCEZFvpDp5JHK/h5mxX8dWlMS5tAZw3uMTMYP9Orbi4O7FHNy9mMHdi2lX2AzQ2E1jogQjIikX79LazSf3oUtxcz6cv5qP5q/mmY8X8tj7XwKwzx6F7NEyj4+/XMOWbcG8aI3dNGxKMCKScrXdc3Nwj7YAVG3dzrSycj6av5oP56/i7dkriF7cqnLLNm4dM4vv9d0z5goDkj5KMCKSFjXdc1MtLyeLQV3bMKhrGy4eujfdrx0Ts92Kis3sf9NY+pa0ZuBeRQzoUsTALm3o1Dp/xxpqurSWekowItJoxJsW3aZ5LqcP6syUr9byxIQFPPTufAD2aNmMAXsVkZeTxeszl1G1dTugS2upogQjIo1GvLGb353UZ0ei2LJtO58tWc+UhWuY8tVapny1hi9XbdzlWJVbtvGHMVpxIJl0VkWk0aht7AYgNzuLvp1b07dza84+NCjrfu2YXcZuAFZWVLH/TWPp3q4F+3dqTZ9Ordi/JPha1DwP0KW1b0IJRkQalUTGbqLFu7TWtkUeZx/ajemLy5m0YM1ON4GWFBVQ3CKXWUvWs3W7Zq3tDiUYEWny4l1au/HE3jslitUbqpixuJwZi9cxvaycV6cvZdv2nfs+lVu2cf3z06jYvJVeHVuy7x4tY94UWt3zyeRtDJRgRKTJS+TSGkBxizyG9GzPkJ7tAeLOWttQtY0bXpi+43nHVvn06tgySDgdWrKkvJL73vqcTVsye1KBEoyIZIT6vLRWUpTPsxcdxpyl65m9bD2zlwaPD+at2jFTLVr1/TpH9dqjxmVwmtKYjxKMiEgc8S6tXTVsP0qKCigpKuCo/fbYUbd123YWrN7IMXe9HfN4Kyo20//m12lXmEePdoXsvUeLnb5OXrCa61+Y0WRWmVaCERGJI9FLa9VysrPYu31h3LXWilvkcdGRPfhi+Qbmraxg7IxlrN6wsMYYKrds47ZXZ/F//TvttJ9OtIbY81GCERGpwe5cWovX8/lt1KQCgDUbqpi3soIvVmzg6v9MjXm8Zes2s9+Nr9G5uIBubVvQpbg5XdsGjy7FLfjkqzXcOLrh9XyUYERE6lki2xhUa9Mij0EtihnUtZi/vjk3Zs+nqCCXHw7eiwWrNrJg9UY+nLeKDVXbdmkXqfpG0gO7taFjq3xysmOv05bMno8SjIhIEtRlG4Nq8Xo+N/1fn51+6bs7qzZUsWDVBhas2sjlz34a83grK6r49oi3yM4yOrbKp6RNAZ3bFNC5qICSNgUsWLWRh9+dz+bdWEKnOjHlddwn5u7CoAQjItJgJDrmY2a0K2xGu8JmDOpazF2vz4l7I+mVw3pRtqaSsrWVLFqzkQlfrGLpuk1sj7W0AV/f57OkfBN7ts6nY+t89mydT4dW+eTnZgNBcolOhLEowYiINCD1OeYTfSNptS3btrO0fBND7ngr5vE2VG1jxGuf7VJe3CKPjq3ymbeigk1xpmNHUoIREWnk6jrbLTc7i72Km8ed7VZSVMDrvz6Cpes2sbR8E0vKN7G0vDL8uomZS9YlFJcSjIhIE1CfPZ+rhvWiRbMc9m5fyN7tC3d53eG3j4u75XUkbf8mIpKhhg8s4bZT+1JSVIAR9FxuO7VvrYnqqmG9KAjHY2qiHoyISAbbnZ5P5CW5JTW0Uw9GRETqbPjAEt679miqln4+KV4bJRgREUkKJRgREUkKJRgREUkKJRgREUkKJRgREUmKlCYYMys2s+fNbIOZLTCzM+K0MzMbYWarwscIM7OI+gFmNsnMNoZfByT6WhERSY1U92DuA6qADsCZwP1m1idGuwuA4UB/oB9wEnAhgJnlAaOBJ4E2wD+B0WF5ja8VEZHUSVmCMbMWwGnAje5e4e7vAi8CZ8Vofg5wl7svcvcy4C7g3LBuKMENone7+2Z3vwcw4OgEXisiIimSyjv59wW2uvuciLJPgSNjtO0T1kW26xNRN9XdIxebnhqWv1bLa3diZhcQ9HgANpvZ9MQ+SkZpB6xMdxANkM7LrnROYmvq56VrvIpUJphCIHoJznKgZZy25VHtCsOxlOi66OPEfW1UUsLdRwIjAcxsorsfmPjHyQw6L7HpvOxK5yS2TD4vqRyDqQBaRZW1AtYn0LYVUBEmiNqOU9NrRUQkRVKZYOYAOWbWM6KsPzAjRtsZYV2sdjOAflEzw/pF1cd7rYiIpEjKEoy7bwBGATebWQszOxw4GXgiRvPHgcvNrMTMOgFXAI+FdaXANuBSM2tmZpeE5eMSeG1NRtb9U2UEnZfYdF52pXMSW8aeF0vllSMzKwYeAY4FVgHXuvvTZjYEeNXdC8N2BowAzgtf+hBwTfVlLjMbGJb1BmYBP3f3KYm8VkREUiOlCUZERDKHlooREZGkUIIREZGkyPgEk+j6aJnGzErNbJOZVYSP2emOKdXM7BIzm2hmm83ssai6Y8zss3A9vLfMLO7NZk1NvPNiZt3MzCN+ZirM7MY0hppS4aSjh8PfI+vN7BMz+25Efcb9zGR8giHx9dEy0SXuXhg+eqU7mDRYDPyBYGLKDmbWjmBG5I1AMTAReCbl0aVPzPMSoSji5+aWFMaVbjnAQoLVSVoDNwDPhok3I39mUnknf4MTsT7a/u5eAbxrZtXro12b1uAk7dx9FICZHQh0jqg6FZjh7s+F9TcBK81sP3f/LOWBplgN5yWjhbdi3BRR9LKZzQcGAW3JwJ+ZTO/BxFsfTT2YwG1mttLM3jOzoekOpgHZab278BfLF+jnptoCM1tkZo+Gf7lnJDPrQPA7ZgYZ+jOT6QmmLuujZZprgB5ACcGNYi+Z2d7pDanBqG09vEy1EhhMsPjhIILz8VRaI0oTM8sl+Oz/DHsoGfkzk+kJpi7ro2UUd//Q3deHWyL8E3gP+F6642og9HMTQ7gNx0R33+ruy4BLgOPMrEn/Eo1mZlkEK5RUEZwDyNCfmUxPMHVZHy3TOcG+OxK13l04lrc3+rmJVn0Xd8b8nglXEnmYYNLQae6+JazKyJ+ZjPmHj6WO66NlDDMrMrNhZpZvZjlmdiZwBMF+Oxkj/Oz5QDaQXX0+gOeB/c3stLD+twR7FDXZwdpI8c6LmR1sZr3MLMvM2gL3AKXuHn1pqCm7H/gWcJK7V0aUZ+bPjLtn9INgyuALwAbgK+CMdMeU7gfQHviYoPu+FpgAHJvuuNJwHm4i+Cs88nFTWPcd4DOgkmAB1m7pjjfd5wX4MTA//L+0hGDh2Y7pjjeF56VreC42EVwSq36cmak/M1qLTEREkiKjL5GJiEjyKMGIiEhSKMGIiEhSKMGIiEhSKMGIiEhSKMGIiEhSKMGINFHh3iynpzsOyVxKMCJJYGaPhb/gox8T0h2bSKpk9H4wIkn2JsHeQpGq0hGISDqoByOSPJvdfWnUYzXsuHx1iZmNCbfQXWBmP4l8sZn1NbM3zazSzFaHvaLWUW3OMbNp4fbFy8zsn1ExFJvZc+GW4POi30MkmZRgRNLn98CLwACCPXceD3eJrF5tdyzBWlYHAacAhxGxTbGZXQg8CDwK9CPYTmF61Hv8FhhNsJLvM8AjZtYlaZ9IJILWIhNJAjN7DPgJwcKHke5z92vMzIGH3P38iNe8CSx195+Y2fnAn4DO7r4+rB8KvAX0dPfPzWwR8KS7x9zeO3yP2939uvB5DsEGexe4+5P192lFYtMYjEjyvANcEFW2NuL7D6LqPgBOCL//FsFy7pEbUr0PbAd6m9k6gt1G/1dLDFOrv3H3rWa2AtgjoehFviElGJHk2ejunyfhuHW57LAl6rmjS+OSIvpBE0mfQ2I8nxV+PwvoG7Xd8GEE/2dnuftyoAw4JulRiuwm9WBEkqeZmXWMKtvm7ivC7081s48JNp86nSBZHBzWPUUwCeBxM/st0IZgQH9URK/oVuAvZrYMGAM0B45x97uS9YFE6kIJRiR5vkOws2OkMqBz+P1NwGkEWwuvAH7q7h8DuPtGMxsG3A18RDBZYDRwWfWB3P1+M6sCrgBGAKuBV5L0WUTqTLPIRNIgnOH1fXf/T7pjEUkWjcGIiEhSKMGIiEhS6BKZiIgkhXowIiKSFEowIiKSFEowIiKSFEowIiKSFEowIiKSFP8f9o4JO8yw41UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history['lr'], 'o-')\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Schedule function can take the current learning rate as a second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class:`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: that the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "lr0 = 0.01\n",
    "model.compile(optimizer=keras.optimizers.Nadam(lr=lr0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 17s 9ms/step - loss: 1.1117 - accuracy: 0.7369 - val_loss: 0.7126 - val_accuracy: 0.8212\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.7106 - accuracy: 0.7943 - val_loss: 0.7155 - val_accuracy: 0.7956\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.6159 - accuracy: 0.8172 - val_loss: 0.6416 - val_accuracy: 0.8258\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.5264 - accuracy: 0.8387 - val_loss: 0.5194 - val_accuracy: 0.8320\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4691 - accuracy: 0.8510 - val_loss: 0.6287 - val_accuracy: 0.8312\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.4416 - accuracy: 0.8591 - val_loss: 0.4797 - val_accuracy: 0.8514\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4030 - accuracy: 0.8700 - val_loss: 0.4745 - val_accuracy: 0.8594\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3551 - accuracy: 0.8799 - val_loss: 0.4962 - val_accuracy: 0.8696\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3547 - accuracy: 0.8854 - val_loss: 0.4854 - val_accuracy: 0.8714\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3117 - accuracy: 0.8972 - val_loss: 0.4779 - val_accuracy: 0.8682\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2830 - accuracy: 0.9045 - val_loss: 0.4988 - val_accuracy: 0.8706\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2734 - accuracy: 0.9049 - val_loss: 0.4549 - val_accuracy: 0.8846\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2454 - accuracy: 0.9142 - val_loss: 0.5408 - val_accuracy: 0.8864\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2275 - accuracy: 0.9200 - val_loss: 0.4809 - val_accuracy: 0.8850\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2144 - accuracy: 0.9250 - val_loss: 0.4577 - val_accuracy: 0.8814\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1948 - accuracy: 0.9320 - val_loss: 0.4454 - val_accuracy: 0.8874\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.1800 - accuracy: 0.9385 - val_loss: 0.4572 - val_accuracy: 0.8902\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.1641 - accuracy: 0.9428 - val_loss: 0.5216 - val_accuracy: 0.8848\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.1579 - accuracy: 0.9467 - val_loss: 0.6263 - val_accuracy: 0.8874\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.1451 - accuracy: 0.9496 - val_loss: 0.5806 - val_accuracy: 0.8928\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.1367 - accuracy: 0.9539 - val_loss: 0.6347 - val_accuracy: 0.8890\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.1252 - accuracy: 0.9583 - val_loss: 0.6276 - val_accuracy: 0.8904\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1210 - accuracy: 0.9610 - val_loss: 0.6551 - val_accuracy: 0.8864\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.1095 - accuracy: 0.9645 - val_loss: 0.7265 - val_accuracy: 0.8906\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.1099 - accuracy: 0.9656 - val_loss: 0.7179 - val_accuracy: 0.8886\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "s = 20 * len(X_train) // 32 # no. of steps in 20 epochs (batch_size 32)\n",
    "exp_decay = ExponentialDecay(s=s)\n",
    "history = model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), epochs=n_epochs, callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = n_epochs * len(X_train) // 32\n",
    "steps = np.arange(n_steps)\n",
    "lrs = lr0 * 0.1**(steps/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps, lrs, \"-\", linewidth=2)\n",
    "plt.axis([0, n_steps - 1, 0, lr0 * 1.1])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling (per batch)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Piecewise constant scheduling`**\n",
    "\n",
    "> Use a constant learning rate for a number of epochs (e.g., $\\eta_0$ $=$ $0.1$ for $5$ epochs), then a smaller learning rate for another number of epochs (e.g., $\\eta_0 = 0.001$ for $50$ epochs), and so on. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates and how long to use each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "1719/1719 [==============================] - 16s 8ms/step - loss: 1.0835 - accuracy: 0.7315 - val_loss: 0.8893 - val_accuracy: 0.6592\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.8970 - accuracy: 0.7333 - val_loss: 0.8117 - val_accuracy: 0.7402\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.7718 - accuracy: 0.7573 - val_loss: 1.3021 - val_accuracy: 0.6368\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.01.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.8885 - accuracy: 0.7238 - val_loss: 0.9946 - val_accuracy: 0.6994\n",
      "Epoch 5/25\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.01.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.8518 - accuracy: 0.7128 - val_loss: 0.9286 - val_accuracy: 0.7058\n",
      "Epoch 6/25\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.6570 - accuracy: 0.7659 - val_loss: 0.6825 - val_accuracy: 0.8048\n",
      "Epoch 7/25\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.5198 - accuracy: 0.8266 - val_loss: 0.6294 - val_accuracy: 0.8248\n",
      "Epoch 8/25\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4992 - accuracy: 0.8411 - val_loss: 0.6416 - val_accuracy: 0.8414\n",
      "Epoch 9/25\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4730 - accuracy: 0.8501 - val_loss: 0.6172 - val_accuracy: 0.8378\n",
      "Epoch 10/25\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4806 - accuracy: 0.8475 - val_loss: 0.6226 - val_accuracy: 0.8498\n",
      "Epoch 11/25\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4401 - accuracy: 0.8651 - val_loss: 0.6080 - val_accuracy: 0.8536\n",
      "Epoch 12/25\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4506 - accuracy: 0.8656 - val_loss: 0.6149 - val_accuracy: 0.8478\n",
      "Epoch 13/25\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.4312 - accuracy: 0.8713 - val_loss: 0.6401 - val_accuracy: 0.8502\n",
      "Epoch 14/25\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4131 - accuracy: 0.8738 - val_loss: 0.6880 - val_accuracy: 0.8358\n",
      "Epoch 15/25\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.005.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4352 - accuracy: 0.8714 - val_loss: 0.7495 - val_accuracy: 0.8512\n",
      "Epoch 16/25\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3148 - accuracy: 0.9014 - val_loss: 0.5181 - val_accuracy: 0.8844\n",
      "Epoch 17/25\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2577 - accuracy: 0.9113 - val_loss: 0.5099 - val_accuracy: 0.8828\n",
      "Epoch 18/25\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2426 - accuracy: 0.9166 - val_loss: 0.5446 - val_accuracy: 0.8772\n",
      "Epoch 19/25\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2298 - accuracy: 0.9217 - val_loss: 0.5364 - val_accuracy: 0.8800\n",
      "Epoch 20/25\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2168 - accuracy: 0.9247 - val_loss: 0.5283 - val_accuracy: 0.8776\n",
      "Epoch 21/25\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2110 - accuracy: 0.9253 - val_loss: 0.5694 - val_accuracy: 0.8856\n",
      "Epoch 22/25\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2039 - accuracy: 0.9293 - val_loss: 0.6169 - val_accuracy: 0.8846\n",
      "Epoch 23/25\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1994 - accuracy: 0.9294 - val_loss: 0.6107 - val_accuracy: 0.8852\n",
      "Epoch 24/25\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.1853 - accuracy: 0.9328 - val_loss: 0.6055 - val_accuracy: 0.8820\n",
      "Epoch 25/25\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.001.\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.1800 - accuracy: 0.9361 - val_loss: 0.6556 - val_accuracy: 0.8846\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_valid_scaled, y_valid), \n",
    "                    epochs=n_epochs, \n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEeCAYAAAC30gOQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtx0lEQVR4nO3deZxcVZ3//9c7+74SEgiaoGACYQmCqDBoRkRwHH5GcX7fGRbBLSrDzwUEYRRFdAZhZAZxEMnXBRD06/INmzjoKPYgig5gJCFCwhogITsJ6c6efH5/nFvkplLVfbvTVZXuej8fj3qk655zb506XalPn+Weo4jAzMysu/VpdAHMzKx3coAxM7OacIAxM7OacIAxM7OacIAxM7OacIAxM7OacICx3Ug6R1Jro8vRHkkh6X2NLocVI+lGST+rwXX3yT4LMzpxzuTsnGMqPbfu4wDThLL/7JE9tkp6WtLXJA3NsvwIeE0jy1jAfsBdtXwBScMlfVnSXyRtlLRcUoukf5BUl/87tfzy68y1Jb1V0q8lrZK0QdJTkm6VNKK7y9UAz5M+T39ucDl6nX6NLoA1zK+As4D+wAnAt4GhwMcjYiOwsYFl61BELKvl9SWNAu4HRgOfB/4H2AL8FXAp8ADwbC3LsLeQdChwD/At4FNAG/Ba4D3AwMaVrHtExHagpp+nphURfjTZA7gR+FnZsf8NvJj9fA7QWpZ+KvAwsAl4BvhnYEAufQDwL8BiYDPwNPCJXPqhwN3AemAF8ENgQpY2FYjc8yHZNe7Jnf9h4Mnc8wDel3v+hdxrLwNuzqUJuAh4ihQ45wNndlBH3yR9kR5QIW0QMCj7eTRwE/BSdu1fAdNyec8BWoETgUeza/4GODCX51XAHcAaYAPwOPD3ufeZf7Rkx98A/BJYBbxMCoZvLitnALOAn2Sv+3T+fVe7doX3+ynghQKfq6nAncC67D0/ABye/8wBnwSWZPX1PWBIZ35P2fsufQ7nAu/Kyj4jS5+RPd8nd87k7NgxBZ+XrnEi8Mfsd/IQ8PqysnwQeC5Lvws4F4hG///emx7uIrOSjaTWzG4knQzcCvwHMI30H+t9pIBSchPwfuB84BDgQ8Da7Pz9gPtIX7DHAm8HhgF3SOoTEY+TgsKM7FrHkb40j5dUamXPAFqqlO804DOk/+AHA39LanGUfCUrzz+SAt0VwA2S3lXlen2AvwdujYgXytMjYlNEbMqe3gi8EXh39t42APdIGpw7ZSBwCane3gyMIrUGSr5JCqp/TarfT5HVXXZNgFNI3TjvzZ4PB75Pan0eS+re+bmksWXF/QIpeB1J6vr8rqRXd3DtcsuAcZL+uko6kvYnBbkATgJeD1wH9M1lOwE4jPT7/1+kFtAnc+nt/p4kDSP9kfI0cAxwMfC1amXqBldkr/F6YDVwqyRlZXkzqdV/HTCdFFi/VMOy9EyNjnB+1P9BWQuG9EWzCvhR9vwcci0YUnC4tOwaM0l/pYr0pR7AKVVe73Lg12XHRmfnHJs9/z/ADdnPXwGuJ3VBvTk79jy7//X9vuzn84GFQP8Krz2UFDxPKDt+DfDzKuXdN7v+pzuox9L7fkvu2EjSX/AfztVlAFNyec4gtbSUPZ8HfLHKa0wm99d1O2UR8GKFOroi97wfKQCe2clr9yW1NgJYTvpr/XxgXC7PP5NakAOqXOPG7HfYN3fsfwO/Kvp7IrXG1gLDculnUrsWzMm5axyfHTsge/5Dci3s7Nhs3ILZ5eEWTPM6RVKrpE2kroz7gP+vSt6jgc9l+VuzGWY/IH0pTACOAnaQun6qnf+WsvOfz9Jem/3bws4WzIzsWi3ADEkHAQdQpQVD6gIaBDwj6TuS/k5SaWzg0CztnrLX/3jutcupyvFyh5De9wOlAxGxjtS1c2gu3+aIWJh7vpTUpTg6e/514POSHpD0FUlHd/TCkvaVdIOkRZLWkboe9wVeXZZ1Xq5s24CVWb7CImJ7RHyA9Dv4DKlb6ELgcUnTsmxHAfdHxJZ2LvWXSOMdJUtzZSnyezoEmBcR+RmOD1A783I/L83+LZV3Kru2kiF1p1mOB/mb132kvwi3AksjYms7efuQmv8/qZC2ssBr9SF1bXymQtry7N8W4PosmByTPR8CnJ69xlNRobsKICKelzSF1Gf+duBq4IuS3sjOmZKnkr4Y86q955Wkv5QP6eB9tSe/TPm2Kml9ACLiO5J+AfwNqfy/l3RFRFzWzvVvAsYDnya19DYDvyYFrrzy9xh0cfZoRCwhdct9X9LngUWkQHNOwUu0V5au/J4q2ZH9m/8joWLXbwH5193ld2bFuLKa14aIeDIiFncQXAD+BEzN8pc/tpH6//uQxhCqnT8NWFzh/PUAsXMc5nOkYLKCFGSOJ/Xpt7RXwEjjIndHxKdJA8HTsnP/QvrynVThtRdXudYOUpfdGZIOKE+XNEjSIOCx7H2/OZc2Ajg8e93CIuKFiJgdEf8vadxkVpZUahH0LTvlr4BvZO95AakFs19nXrOdaxcp70ukLrlh2aG5wF9JKg9wRRX5PT0GHJ6bTg/wprLrlP7gydfF9C6WqT2Pkz5necdWytjMHGCsiMuB0yVdLukwSVMlvU/SVQARsQj4MfBtSadJOlDSCZLOys6/jjQ28SNJb5T0GklvlzRb0vDc6/w3qU/9N9l1nyV9YbyXdgJMdmPohyUdLulA4AOkvz6fyALY14CvSfqgpIMkTZf0MUmzql2TFOieA/4o6QOSpmXnnkWaxTQhIp4gDaDfkL3fw4FbSBMUflCwbpH0dUmnZPUynTToXgpQK0hjEydLGi9pZHZ8EXCmpEMlvYEUENvrnqqk2rXLy/dRSddLeoek12Z1cSUpkN6WZfsmKdj8WNIbsrr6h+z9dKjg7+kHpNbgd7MynET6PeU9Sep+vUzS6yS9gzTNvLtdC7xD0oWSDpb0IdKkBctr9CCQH/V/UGGacln6Oew+TfkdwG9Jg8Qvk6ZtnpdLHwhcRZqCupk01TSffjDwU3ZO510IfINdpzp/jN2nH99IbnA1dzw/yD+T1Be/ljQd90Hgb3N5RRpfKv2VvBL4L+CkDuppJGnw+nHStNhSq+rvgT5ZnkLTlMuuO4PcQHRWD09kr7GSFCwm5vJ/mBTstrNzmvKRpD7/jVldn0WapXdZpTrKHXsW+Ex7165QD0dl77E0fXg18AfgrLJ804CfkyZ/rAd+DxxW7TMHXAY82pnfE2nG3p+y9EdIXWqvDPJneY4jtao3Zp+L0lTmzg7yV50okB37ICmYbSRNfLgA2Njo/99706M0i8XMzPaApH8H3h4Rhze6LHsLD/KbmXWBpAtJLaxW0uSMjwH/1NBC7WXcgjEz6wJJPyJ1p40krW5xA/D18JfqKxxgzMysJjyLzMzMasJjMJlRo0bFQQcd1Ohi7HXa2toYOnRoxxmbjOtld66Tynp7vTz88MOrImJcpTQHmMz48eN56KGHGl2MvU5LSwszZsxodDH2Oq6X3blOKuvt9SKp4g3L4C4yMzOrEQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOrCQcYMzOriboGGEljJN0mqU3SYkmnV8knSVdKWp09rpSkXPpsSQsl7ZB0ToXzPy1pmaSXJX1X0sCOyvbsyzs4/qv3cvvcJYXey+1zl3D8V+/lwIvv7pXnmZntqXq3YK4DtgDjgTOA6yVNq5BvFjATOBI4AjgV+Ggu/RHgXOBP5SdKOhm4GDgRmAS8BvhSkcItWbuRS+bM7/BL+Pa5S7hkznyWrN1I9MLzzMy6gyKiPi8kDQVeAg6LiEXZse8DSyLi4rK8vwdujIjZ2fMPAR+JiDeV5bsf+HZE3Jg79gPg2Yj4p+z5icCtETGhvfIN3O/g2O/sawDo20fsN3JQ1bwvrtvE9h2711tPOW/iqMH87uK3VT0vr7fvxtdVrpfduU4q6+31IunhiDimUlo9t0x+HbCtFFwyjwBvrZB3WpaWz1eppVPJNOCOsnPHSxobEavzGSXNIrWWGDDhoFeOb98RvPDSxoIvt1NPOW/J2o20tLQUytva2lo4bzNxvezOdVJZM9dLPQPMMODlsmPrgOFV8q4ryzdMkqLjJlelc8leZ5cAk7WQZkNqwZSOjx8xkJ9+7LiqL/C+b/2e5S9v3u14Tzlv4qjBhf+i6u1/fXWV62V3rpPKmrle6hlgWoERZcdGAOsL5B0BtBYILtXOpcrr7GZw/75c8s5DeNWYIVXzXPLOQ7hkznw2bt3eI8+78OQpVc8xM+su9RzkXwT0k3Rw7tiRwIIKeRdkaR3lq6TSucvLu8cqmThqMFe893BmHjWx3Xwzj5rIFe89nImjBqMecN6wgenviBGD+hU6z8ysO9StBRMRbZLmAJdL+jAwHXg3UKmP52bgfEk/BwK4APhGKVHSAFJwFNBf0iBgS0TsyM69UdKtwFLg88CNHZVv8og+hQe+IX15d+WLuhHnbdy6nUvmzOekQyc4uJhZ3dR7mvK5wGBgBfBD4OMRsUDSCZJac/luAO4C5gOPAndnx0p+CWwkBafZ2c9vAYiIe4CrgN8AzwGLgS/W8D3t9SaNTd1oi1e3NbgkZtZM6jkGQ0SsId3fUn78t6TB+dLzAC7KHpWuM6OD1/k34N/2oKi9yoH7DAXg2dUbGlwSM2smXiqmCYwfPoiB/fqwqnUzrZu3Nbo4ZtYkHGCaQJ8+cjeZmdWdA0yTmDQ2dZMtdjeZmdWJA0yTmJy1YJ51C8bM6sQBpkmUWjDPrnKAMbP6cIBpEpPHeiaZmdWXA0yT8CC/mdWbA0yT2H/UYAb07cPylzezYYunKptZ7TnANIm+fcSrxgwG4Lk17iYzs9pzgGkir4zDrHKAMbPac4BpIjvvhfE4jJnVngNME5m8j++FMbP6cYBpIpPcRWZmdeQA00Qme6qymdWRA0wTmThqMP36iKXrNrEpt42ymVktOMA0kX59+/CqMakV87ynKptZjTnANJlJryx66QBjZrXlANNkJnuqspnViQNMkym1YJ7xqspmVmMOME1msjceM7M6cYBpMpO88ZiZ1YkDTJM5YPQQ+giWrt3I5m2eqmxmteMA02QG9OvDAaOHsCPghZc2Nro4ZtaLOcA0IW8+Zmb14ADThLxsv5nVgwNME/JAv5nVgwNME3qlBeOpymZWQw4wTai0L4zHYMyslhxgmtABo4cgpVlkW7fvaHRxzKyXcoBpQoP692X/kYPZviNY4qnKZlYjdQ0wksZIuk1Sm6TFkk6vkk+SrpS0OntcKUm59OmSHpa0Ift3ei5toKRvSVouaY2kuyRNrMPb61G8fbKZ1Vq9WzDXAVuA8cAZwPWSplXINwuYCRwJHAGcCnwUQNIA4A7gFmA0cBNwR3Yc4JPAm7Pz9gdeAr5Rm7fTc03ymmRmVmN1CzCShgKnAZdGRGtE3A/cCZxVIfvZwNUR8UJELAGuBs7J0mYA/YBrImJzRFwLCHhbln4g8IuIWB4Rm4AfAZWCWFOb7FWVzazG+tXxtV4HbIuIRbljjwBvrZB3WpaWzzctlzYvIiKXPi87fg/wHeDrkvYH1pJaSv9ZqUCSZpFaS4wbN46WlpbOvaMebP3ybQD86YnnaWlZWTVfa2trU9VLUa6X3blOKmvmeqlngBkGvFx2bB0wvEredWX5hmXjMOVp5dd5AngeWAJsB+YD51UqUETMBmYDTJkyJWbMmFHwrfR8+y1bzzfm3kdrDKK9993S0tJuerNyvezOdVJZM9dLPcdgWoERZcdGAOsL5B0BtGatlo6ucx0wEBgLDAXmUKUF08xePSZ1kT3/0ga2eaqymdVA4QAj6Z2SfibpL5JelR37sKQTC15iEdBP0sG5Y0cCCyrkXZClVcq3ADgiP6uMNKBfSp8O3BgRayJiM2mA/1hJ+xQsZ1MYPKAvE0YMYuv24MV1mxpdHDPrhQoFGElnAD8mdT8dCPTPkvoCFxW5RkS0kVoTl0saKul44N3A9ytkvxk4X9LEbCzlAuDGLK2F1PX1iWxKcqn7697s3weB90saKak/cC6wNCJWFSlnM/FUZTOrpaItmIuAj0TEp4FtueN/ILUYijoXGAysAH4IfDwiFkg6QVJrLt8NwF2k8ZNHgbuzY0TEFtIU5veTBvE/CMzMjgN8BthECoYrgb8B3tOJMjYNr0lmZrVUdJD/YOCBCscrjYdUFRFrSMGh/PhvSYP3pedBCmoVW0cRMRc4ukraatLMMevApFeW7XcLxsy6X9EWzFLSNONybwGe6r7iWD1N9sZjZlZDRQPMbODabNwE4FWSzgauAq6vScms5ia5i8zMaqhQF1lEXCVpJPBfwCDgN8Bm4GsRcV0Ny2c1VNp47LnVG9i+I+jbRx2cYWZWXOFpyhHxOWAf4FjgTcC4iLi0VgWz2hs6sB/jhg9ky/YdLHvZU5XNrHsVnab8XUnDI2JDRDwUEf8TEa3ZdOPv1rqQVjuvjMN4oN/MulnRFszZpOnF5QaTpgtbD+WpymZWK+2OwUgaQ1qpWMBoSfl7YPoC7wKW1654VmuT9ykFGLdgzKx7dTTIvwqI7PGXCukBfLG7C2X1Uxro970wZtbdOgowf01qvdxL2stlTS5tC7A4IpbWqGxWB5O98ZiZ1Ui7ASYi/htA0oHA8xHhZXd7mVeXBvnXtLFjR9DHU5XNrJsUvQ9mMUC28OSrgQFl6fd1f9GsHkYM6s/YoQNY3baFFes3M2HkoEYXycx6iUIBJgssPyAtDROkbrP8jpJ9u79oVi+Txg5hddsWnl3d5gBjZt2m6DTla0hL5B8KbABOAP4OeAw4pSYls7opzSTzmmRm1p2Krqb8VuBdEfG4pABWRsTvJG0GvkxaQsZ6qNJA/zOrPNBvZt2naAtmMGnKMqSZZPtmP/+FtJuk9WCTvKqymdVA0QDzODA1+/nPwMckTQL+EVhSg3JZHflufjOrhaJdZF8HJmQ/Xw7cA/wDaUXls2tQLqujnffCtBERSJ6qbGZ7rug05VtzP/9J0mRSi+Y573Xf840c0p9RQ/qzdsNWVrZuZt/hnklmZnuu8HL9edmqyn8C2iRd3M1lsgaY5Dv6zaybdRhgJO0j6V2S3iGpb3asv6RPAc8Cn6ltEa0eJntNMjPrZh2tpnwccDcwknRj5YOSzgFuA/qTpih7P5heYOdAvwOMmXWPjlowXwZ+QZqKfA1pN8ufAVcAB0fEf0SE+1R6gcn7ZC0Yd5GZWTfpKMAcCXw5Ih4FLiW1Yi6JiJsjIto/1XqSSWN9N7+Zda+OAswYYCWkgX3SMjFza10oq79Xpiqv2oD/djCz7lBkmnJpJ8vSApcjsp0uXxERayqeaT3G6CH9GT6oH+s3bWNN2xbGDhvY6CKZWQ9XJMDkd7IU8GDZ88CrKfd4kpg8dijzl6zj2dUbHGDMbI8V2dHSmsSksUOYv2Qdi1e3cfSk0Y0ujpn1cIV2tLTmcGC2bL/vhTGz7tClO/mtd5rkRS/NrBvVNcBIGiPpNkltkhZLOr1KPkm6UtLq7HGlciswSpou6WFJG7J/p5ed/3pJ90lqlbRc0idr/NZ6hclett/MulG9WzDXAVuA8cAZwPWSplXINwuYSboP5wjgVOCjAJIGAHcAtwCjgZuAO7LjSNqHtNrzDcBY4CDglzV7R72IWzBm1p3qFmAkDQVOAy6NiNaIuB+4EzirQvazgasj4oWIWAJcDZyTpc0gjR1dExGbI+Ja0my2t2Xp5wO/iIhbs/T1EfFYzd5YL7LPsAEMHdCXdRu3snbDlkYXx8x6uKL7wXSH1wHbImJR7tgjpO2Yy03L0vL5puXS5pWtJDAvO34P8CZgvqTfk1ovfwT+MSKeK38RSbNIrSXGjRtHS0tLF95W7zJ2YNC2Beb88re8ZlRfWltbXS8VuF525zqprJnrpVCAkVRtQcsANgFPAj+KiKXtXGYY8HLZsXXA8Cp515XlG5aNw5SnlV/nAOD1wEnAfOAq4IfA8bsVPmI2MBtgypQpMWPGjHaK3xwOW/Iwz81fxtjJU5kxfSItLS24Xnbnetmd66SyZq6Xoi2YccAJwA7g0ezYYaSuqYeB9wKXSzohIv5c5RqtwIiyYyOA9QXyjgBaIyIkdXSdjcBtEfEggKQvAaskjYyI8sBkZUrjMM94qrKZ7aGiYzC/A/4TOCAi3hIRbyG1FH5OGkCfRFrW/+p2rrEI6Cfp4NyxI4EFFfIuyNIq5VsAHJGfVUaaCFBKn0dqWZV4Ya1OONAbj5lZNykaYD4JXJ5fmj/7+Z+BT0fEFuBKYHq1C0REGzCH1NIZKul44N3A9ytkvxk4X9JESfsDFwA3ZmktwHbgE5IGSjovO35v9u/3gPdkU5n7k1aBvt+tl2ImlTYe81RlM9tDRQPMMGC/CscnZGmQxlc66nI7FxgMrCCNi3w8IhZIOiHr+iq5AbiLNIbyKKl1dANAFsxmAu8H1gIfBGZmx4mIe4F/ys5ZQRror3i/je1u8j5uwZhZ9yg6BnMb8B1JF7Fzscs3kAbQ52TPjyV1g1WVrbo8s8Lx37IzUJHNELsoe1S6zlzg6HZe53rg+vbKYpXtO3wgg/r3YU3bFtZt3Nro4phZD1a0BfMx0s6WtwBPZY9bSNOCz83yPAZ8pLsLaPVVWlUZ4Dm3YsxsDxRqwWTjLR+TdAHw2uzwU9m4SinPn7u/eNYIk8YO4fFl63l2dVvFOeRmZkV06kbLLKDMq1FZbC8xObd98mFeDtXMuqjojZaDSDPJTgT2paxrLSKO6P6iWaOUBvqfWbWBw/ZtcGHMrMcq2oL5JvAe4CfA7/G9Jb3apPyqyg4wZtZFRQPMTODvIuJXNSyL7SUm77Kqcj2XqzOz3qRoD/sG4PlaFsT2HhNGDGJAvz6sat3Mxm1urJpZ1xQNMFeR7qxXhzmtx+vTR0wak7rJVmzY0eDSmFlPVbT/4yTSYpenSPoLsMsdeBHx/3R3wayxJo0dyhMrWlmxwS0YM+uaogFmFelufmsSpe2Tl7sFY2ZdVPRGyw/UuiC2d1mbLRPz00VbeeCr93LhyVOYedTEDs+7fe4S/vUXC1m6diP7jxrca89bsnYjE//gejFrj6cI2W5un7uEO/+8c++4JWs3cvGceWzdvoNTj9y/6nl3PbKUS+94lE1bd/i8XnLeJXPmAzjIWJdo152HcwnSPOCtEfGSpPm0c+9Lb7jRcsqUKbFw4cJGF2OvcPxX72XJ2o2NLobtJSaOGszvLn5bh/maeefG9vT2epH0cEQcUymtvRbM/wU2Zz//tNtLZXutpe0El4H9qk883Lyt+niNz+u557X3eTBrT9UAExFfqvSz9X77jxpcsQXT0V+y1Vo+Pq9nn7f/qMFVzzFrj5cytN1cePIUBvfvu8uxwf37cuHJU3xeLz9vUP9dvxKKnGdWTdHFLseQtkeuttjliO4vmjVKaUD3ldlSBWcT5c/rzCyknnheb66X83/8Z3YEjB8xkEveeYgH+K3Lqg7y75JJug04CpgNLKVswD8ibqpJ6erIg/yV9fYByq7qzfXy/u/+D/ctWsm3zjyaUw6bUPi83lwne6K310tXB/nzTgROiog/dl+xzGxvNHXCcO5btJKFy9Z3KsCYlSs6BrMCaK1lQcxs7zBlfNrHdOHylxtcEuvpigaYzwGXSxpWy8KYWeNNmZACzOPL1je4JNbTFe0i+zwwGVghaTG7L3bZ42+0NLPkoH2H0bePeHZVG5u2bmdQ2Yw0s6KKBhjfaGnWJAb178uB+wzlyRWtPLG8lcMPGNnoIlkP1WGAkdQfGApcFxGLa18kM2u0KROG8+SKVh5f9rIDjHVZh2MwEbEV+DjgzcbMmsTU0kC/x2FsDxQd5P8l0PFqd2bWK5QG+hcud4Cxris6BvNr4F8kHQE8DLTlEyNiTncXzMwaZ+qEtDiHZ5LZnigaYP4j+/cTFdIC8DQTs17kgNGDGTKgLyvXb2Z162bGDhvY6CJZD1Soiywi+rTzcHAx62X69NHObjK3YqyLvJqymVU01Tdc2h4qHGAkjZZ0uqSLJX0h/+jENcZIuk1Sm6TFkk6vkk+SrpS0OntcKUm59OmSHpa0Ift3eoVrDJD0mKQXipbPzHaa4plktoeKLtf/JuBu0g6X44AlwH7Z82eBywu+3nXAFmA8MB24W9IjEbGgLN8sYCZwJGmM57+AZ4BvSRoA3AFcA3wT+Chwh6SDI2JL7hoXAiuB4QXLZmY5U0oD/Z5JZl1UtAXzr8CtwERgE2nK8quBh4Ari1xA0lDgNODSiGiNiPuBO4GzKmQ/G7g6Il6IiCXA1cA5WdoMUmC8JiI2R8S1pHt0XplGLelA4EzgioLvz8zKlLrInli+nh07Ot7Ww6xc0VlkRwAfioiQtB0YGBFPS/os8ANS8OnI64BtEbEod+wR4K0V8k7L0vL5puXS5sWuG9nMy47fkz3/BvBPQLubiUuaRWotMW7cOFpaWgq8jebS2trqeqmgWepl1ECxdvN2fnrPb9h3SPt/jzZLnXRWM9dL0QCT73paDkwCHiMt4b9/wWsMA8rX/15H5S6sYVlaPt+wbBymPG2X60h6D9A3Im6TNKO9AkXEbNImakyZMiV686ZAXdXbN0vqqmaplyOeTpuPjXj1oczoYG+YZqmTzmrmeinaRfYn4A3Zzy3AVySdDVxLaj0U0QqUb608AqjUwVuedwTQmrVaql4n64a7isr365hZJ031VGXbA53ZD2Zp9vPnSYPn3wBGk3UxFbAI6Cfp4NyxI4HyAX6yY0dWybcAOCI/q4zUhbcAOJi0rcBvJS0D5gD7SVomaXLBcppZxpuP2Z4o1EUWEQ/lfl4JvLOzLxQRbZLmkDYu+zBpFtm7geMqZL8ZOF/Sz0mzyC4gBTRILajtwCckfQv4SHb8XmAH8KrcdY4jrULwelJQNLNO8OZjtic6daOlpGMk/a+sKwpJQyUVHccBOBcYTNqC+YfAxyNigaQTJOW3ZL4BuAuYDzxKmiJ9A0A2FXkm8H5gLfBBYGZEbImIbRGxrPQA1gA7sufbO/NezWz3zcfMOqPofTDjSfeeHEtqURwMPA38G2na8ieLXCci1pCCQ/nx35IG70vPA7goe1S6zlzg6AKv1wIcUKRsZra7Qf37MnnsEJ5a2caTK1o5bKL3hrHiirZg/p00e2wssCF3/CfAO7q7UGa295i6X5pT89iLHoexzikaYE4EPhcRL5Udf4p0w6WZ9VLefMy6qmiAGcyu98KUjCN1kZlZL+XNx6yrigaY+9i5VAtASOoLfJa0GZmZ9VLefMy6qugMsIuA/5b0BmAgaW2wacBI4Pgalc3M9gL5zcfWtG1hzNABjS6S9RBFNxz7C3A48Hvgl8Ag0gD/URHxVO2KZ2aN1qePeN340v0wHui34grfB5PdS/LFiPjbiPibiPg8MEDSj2tYPjPbCxyyXxZgXnQ3mRW3pztajiItwW9mvZg3H7Ou8JbJZtYhbz5mXeEAY2Yd8uZj1hUOMGbWodFDB7Dv8IFs2LKd51/a0PEJZnQwTVnSnR2cX74vi5n1UlMmDGfF+s08vmw9k8YObXRxrAfoqAWzuoPHM6Sl9c2slzskW5PMM8msqHZbMBHxgXoVxMz2bt58zDrLYzBmVog3H7POcoAxs0K8+Zh1lgOMmRVS2nxsR8CTK1o7PsGangOMmRXmlZWtMxxgzKyw0g2Xj3t3SyvAAcbMCvPmY9YZDjBmVpi7yKwzHGDMrLDyzcfM2uMAY2aFefMx6wwHGDPrlNJAv/eGsY44wJhZp+ycSeYAY+1zgDGzTvHmY1aUA4yZdYo3H7OiHGDMrFO8+ZgV5QBjZp3mlZWtiLoGGEljJN0mqU3SYkmnV8knSVdKWp09rpSkXPp0SQ9L2pD9Oz2XdqGkRyWtl/SMpAvr8NbMmopnklkR9W7BXAdsAcYDZwDXS5pWId8sYCZwJHAEcCrwUQBJA4A7gFuA0cBNwB3ZcQAB78/STgHOk/T3NXo/Zk1p5x39vhfGqqtbgJE0FDgNuDQiWiPifuBO4KwK2c8Gro6IFyJiCXA1cE6WNoO0E+c1EbE5Iq4lBZW3AUTEVRHxp4jYFhELScHo+Bq+NbOm4y4yK6LdLZO72euAbRGxKHfsEeCtFfJOy9Ly+abl0uZFRH76yrzs+D35i2TdaicAN1QqkKRZpNYS48aNo6Wlpeh7aRqtra2ulwqavV62bA/6CJ5Z2cYvf/0bBvRV09dJNc1cL/UMMMOA8vb0OmB4lbzryvINywJGeVp717mM1Er7XqUCRcRsYDbAlClTYsaMGe2+gWbU0tKC62V3rhc48M8tPLWyjf2nvp7DJo50nVTRzPVSzzGYVmBE2bERQKU2dnneEUBr1mopdB1J55HGYt4VEZv3oNxmVoFXVraO1DPALAL6STo4d+xIYEGFvAuytEr5FgBH5GeVkSYCvHIdSR8ELgZOjIgXuqHsZlbmlb1hPNBvVdQtwEREGzAHuFzSUEnHA+8Gvl8h+83A+ZImStofuAC4MUtrAbYDn5A0MGupANwLIOkM4F+AkyLi6Vq9H7NmN9UD/daBek9TPhcYDKwAfgh8PCIWSDpBUmsu3w3AXcB84FHg7uwYEbGFNIX5/cBa4IPAzOw4wFeAscCDklqzx7dq/cbMmo27yKwj9RzkJyLWkIJD+fHfkgbvS88DuCh7VLrOXODoKmkHdkdZzax93nzMOuKlYsysS7z5mHXEAcbMusxLxlh7HGDMrMumOMBYOxxgzKzLSgP9jznAWAUOMGbWZbtsPhbefMx25QBjZl2W33xs1UYHGNuVA4yZ7ZHSOMzz63c0uCS2t3GAMbM9Uuome8EBxso4wJjZHpmSDfS/0OoAY7tygDGzPTLVXWRWRV2XijGz3uexF9Nd/MvaguOu+DUXnTKVmUdN7PC82+cu4V9/sZClazey/6jBXHjylF553pK1G5n4h3v3+nJ29bwBEw6quGwXOMCY2R64fe4SvnDHzh03lq7bxCVz5gO0+yV1+9wlXDJnPhu3bgdgydqNPq8Hn1eNwnPXgbSj5cKFCxtdjL1OM+/G1x7XS3L8V+9lydqNFdMG9KveA79lW/XuNJ/Xs8578aZPsfnFJ1Qpn1swZtZlS6sEF2j/y6s9Pq9nn5fnAGNmXbb/qMEVWzD7jxzEvZ+ZUfW8t32thaXrNvm8XnZeOc8iM7Muu/DkKQzu33eXY4P79+WiU6YyqH/fqo+LTpnq83rZeZW4BWNmXVYaCH5ltlTBWUj58zoze6knntfb6+XFdvJ5kD/jQf7KPJhdmetld66Tynp7vUh6OCKOqZTmLjIzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6uJugYYSWMk3SapTdJiSadXySdJV0panT2ulKRc+nRJD0vakP07vei5ZmZWH/VuwVwHbAHGA2cA10uaViHfLGAmcCRwBHAq8FEASQOAO4BbgNHATcAd2fF2zzUzs/qpW4CRNBQ4Dbg0Iloj4n7gTuCsCtnPBq6OiBciYglwNXBOljaDtI/NNRGxOSKuBQS8rcC5ZmZWJ/XccOx1wLaIWJQ79gjw1gp5p2Vp+XzTcmnzYteNbOZlx+/p4NxdSJpFavEAbJb0aLG30lT2AVY1uhB7IdfL7lwnlfX2eplULaGeAWYY8HLZsXXA8Cp515XlG5aNpZSnlV+n6rllQYmImA3MBpD0ULVNc5qZ66Uy18vuXCeVNXO91HMMphUYUXZsBLC+QN4RQGsWIDq6TnvnmplZndQzwCwC+kk6OHfsSGBBhbwLsrRK+RYAR5TNDDuiLL3auWZmVid1CzAR0QbMAS6XNFTS8cC7ge9XyH4zcL6kiZL2By4AbszSWoDtwCckDZR0Xnb83gLntmd2599VU3C9VOZ62Z3rpLKmrRfVs+dI0hjgu8BJwGrg4oj4gaQTgP+MiGFZPgFXAh/OTv028NlSN5eko7JjhwKPAR+KiLlFzjUzs/qoa4AxM7Pm4aVizMysJhxgzMysJpo+wBRdH63ZSGqRtElSa/ZY2Ogy1Zuk8yQ9JGmzpBvL0k6U9Hi2Ht5vJFW92ay3qVYvkiZLitxnplXSpQ0sal1lk46+k32PrJf0Z0nvzKU33Wem6QMMxddHa0bnRcSw7DGl0YVpgKXAV0gTU14haR/SjMhLgTHAQ8CP6l66xqlYLzmjcp+bL9exXI3WD3ietDrJSODzwI+zwNuUn5l63sm/18mtj3ZYRLQC90sqrY92cUMLZw0XEXMAJB0DHJBLei+wICJ+kqVfBqySNDUiHq97QeusnXppatmtGJflDv1M0jPA0cBYmvAz0+wtmGrro7kFk1whaZWk30ma0ejC7EV2We8u+2J5Cn9uShZLekHS97K/3JuSpPGk75gFNOlnptkDTGfWR2s2nwVeA0wk3Sh2l6TXNrZIe42O1sNrVquAN5AWPzyaVB+3NrREDSKpP+m935S1UJryM9PsAaYz66M1lYj4Y0Ssz7ZEuAn4HfA3jS7XXsKfmwqybTgeiohtEbEcOA94h6Re/SVaTlIf0golW0h1AE36mWn2ANOZ9dGaXZD23bGy9e6ysbzX4s9NudJd3E3zPZOtJPId0qSh0yJia5bUlJ+ZpvnFV9LJ9dGahqRRkk6WNEhSP0lnAG8h7bfTNLL3PgjoC/Qt1QdwG3CYpNOy9C+Q9ijqtYO1edXqRdIbJU2R1EfSWOBaoCUiyruGerPrgUOAUyNiY+54c35mIqKpH6Qpg7cDbcBzwOmNLlOjH8A44EFS830t8AfgpEaXqwH1cBnpr/D847Is7e3A48BG0gKskxtd3kbXC/APwDPZ/6UXSQvPTmh0eetYL5OyuthE6hIrPc5o1s+M1yIzM7OaaOouMjMzqx0HGDMzqwkHGDMzqwkHGDMzqwkHGDMzqwkHGDMzqwkHGLNeKtub5X2NLoc1LwcYsxqQdGP2BV/++EOjy2ZWL029H4xZjf2KtLdQ3pZGFMSsEdyCMaudzRGxrOyxBl7pvjpP0t3ZFrqLJZ2ZP1nS4ZJ+JWmjpDVZq2hkWZ6zJc3Pti9eLummsjKMkfSTbEvwp8tfw6yWHGDMGudLwJ3AdNKeOzdnu0SWVtv9BWktq2OB9wDHkdumWNJHgRuA7wFHkLZTeLTsNb4A3EFayfdHwHclvbpm78gsx2uRmdWApBuBM0kLH+ZdFxGflRTAtyPiI7lzfgUsi4gzJX0E+BpwQESsz9JnAL8BDo6IJyW9ANwSERW3985e46sRcUn2vB9pg71ZEXFL971bs8o8BmNWO/cBs8qOrc39/EBZ2gPAu7KfDyEt557fkOr3wA7gUEkvk3Yb/XUHZZhX+iEitklaCexbqPRme8gBxqx2NkTEkzW4bme6HbaWPQ/cNW514g+aWeO8qcLzx7KfHwMOL9tu+DjS/9nHImIFsAQ4sealNOsit2DMamegpAllx7ZHxMrs5/dKepC0+dT7SMHijVnaraRJADdL+gIwmjSgPyfXKvpn4N8lLQfuBoYAJ0bE1bV6Q2ad4QBjVjtvJ+3smLcEOCD7+TLgNNLWwiuBD0TEgwARsUHSycA1wP+QJgvcAXyydKGIuF7SFuAC4EpgDfDzGr0Xs07zLDKzBshmeP1dRPy00WUxqxWPwZiZWU04wJiZWU24i8zMzGrCLRgzM6sJBxgzM6sJBxgzM6sJBxgzM6sJBxgzM6uJ/x+fVO0ltcjWEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\", linewidth=2)\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Performance scheduling`**\n",
    "\n",
    "> Measure the validation error every $N$ steps (just like for early stopping), and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
