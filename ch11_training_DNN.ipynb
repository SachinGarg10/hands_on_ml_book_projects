{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_DNN\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <span style=\"color:#fff; font-family: 'Bebas Neue'; font-size: 1.2em;\">Number of iterations</span> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in Chapter 10, the backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the **`vanishing gradients`** problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the **`exploding gradients`** problem, which surfaces in recurrent neural networks (see Chapter 15). More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio.1 The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks).***\n",
    "\n",
    "***Looking at the logistic activation function (see Figure 11-1), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), 'b-', linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.grid(True)\n",
    "plt.title('Sigmoid activation function', fontsize=16)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction (please check out the paper if you are interested in the mathematical details). It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in Equation 11-1, where $fan_{avg} = (fan_{in} + fan_{out}/2$. This initialization strategy is called ***Xavier initialization or Glorot initialization***, after the paper’s first author.\n",
    "\n",
    "***Equation 11-1. Glorot initialization (when using the logistic activation function)*** <br>\n",
    "Normal distribution with mean $0$ and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$  \n",
    "Or a uniform distribution between $-r$ and $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you replace $fan_{avg}$ with $fan_{in}$ in Equation 11-1, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it *LeCun initialization*. Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book *Neural Networks*: *Tricks of the Trade* (Springer). LeCun initialization is equivalent to Glorot initialization when $fan_{in} = fan_{out}$. It took over a decade for researchers to realize how important this trick is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning.\n",
    "\n",
    "Some papers have provided similar strategies for different activation functions. These strategies differ only by the scale of the variance and whether they use $fan_{avg}$ or $fan_{in}$, as shown in Table 11-1 (for the uniform distribution, just compute $r = \\sqrt{3\\sigma^2}$). The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called `He initialization`, after the paper’s first author. The SELU activation function will be explained later in this chapter. It should be used with LeCun initialization (preferably with a normal distribution, as we will see)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Initialization parameters of each type of activation function](images/training_DNN/initialization_parameters_activation_func.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses ***Glorot*** initialization with a uniform distribution. When creating a layer, you can change this to ***He*** initialization by setting ***kernel_initializer='he_uniform'*** or ***kernel_initializer='he_normal'*** like this:\n",
    "``` python\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "```\n",
    "\n",
    "If you want ***He*** initialization with a uniform distribution but based on ***fan***$_{avg}$ rather than ***fan***$_{in}$, you can use the ***VarianceScaling*** initializer like this:\n",
    "``` python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fefab146358>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fefaacc0b38>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions\n",
    "\n",
    "One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks—in particular, ***`the ReLU activation function, mostly because it does not saturate for positive values (and because it is fast to compute)`***\n",
    "\n",
    "**Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.**\n",
    "\n",
    "***To solve this problem, you may want to use a variant of the ReLU function, such as the `leaky ReLU`. This function is defined as $LeakyReLU_α(z) = max(αz, z)$ (see Figure 11-2). The hyperparameter $α$ defines how much the function `“leaks”`: it is the slope of the function for $z < 0$ and is typically set to $0.01$. `This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up.` A [2015 paper](https://homl.info/49) `compared several variants of the ReLU activation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function.` In fact, setting $α = 0.2$ (a huge leak) seemed to result in better performance than $α = 0.01$ (a small leak). The paper also evaluated the `randomized leaky ReLU (RReLU)`, where $α$ is picked randomly in a given range during training and is fixed to an average value during testing. `RReLU also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set).` Finally, the paper evaluated the `parametric leaky ReLU (PReLU)`, where $α$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). `PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 4ms/step - loss: 1.8506 - accuracy: 0.4323 - val_loss: 1.0055 - val_accuracy: 0.7110\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.9354 - accuracy: 0.7177 - val_loss: 0.7534 - val_accuracy: 0.7652\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.7437 - accuracy: 0.7588 - val_loss: 0.6670 - val_accuracy: 0.7880\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6584 - accuracy: 0.7847 - val_loss: 0.6064 - val_accuracy: 0.8044\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6166 - accuracy: 0.7983 - val_loss: 0.5686 - val_accuracy: 0.8200\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5751 - accuracy: 0.8113 - val_loss: 0.5419 - val_accuracy: 0.8258\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5471 - accuracy: 0.8201 - val_loss: 0.5200 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5235 - accuracy: 0.8277 - val_loss: 0.5107 - val_accuracy: 0.8306\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5164 - accuracy: 0.8275 - val_loss: 0.4916 - val_accuracy: 0.8380\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4968 - accuracy: 0.8321 - val_loss: 0.4826 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 3ms/step - loss: 1.6975 - accuracy: 0.4971 - val_loss: 0.9259 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.8709 - accuracy: 0.7245 - val_loss: 0.7308 - val_accuracy: 0.7626\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7213 - accuracy: 0.7620 - val_loss: 0.6566 - val_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6449 - accuracy: 0.7881 - val_loss: 0.6005 - val_accuracy: 0.8044\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6078 - accuracy: 0.8003 - val_loss: 0.5657 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5693 - accuracy: 0.8119 - val_loss: 0.5407 - val_accuracy: 0.8236\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5428 - accuracy: 0.8195 - val_loss: 0.5197 - val_accuracy: 0.8316\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5194 - accuracy: 0.8284 - val_loss: 0.5115 - val_accuracy: 0.8308\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5129 - accuracy: 0.8271 - val_loss: 0.4917 - val_accuracy: 0.8376\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4941 - accuracy: 0.8316 - val_loss: 0.4827 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', activation='relu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.plot(z, leaky_relu(z, 0.05), 'b-', linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha='center')\n",
    "plt.grid(True)\n",
    "plt.title('Leaky ReLU activation function', fontsize=16)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the leaky ReLU activation function, create a ***`LeakyReLU`*** layer and add it to your model just after the layer you want to apply it to:\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential([\n",
    "    [...]\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    [...]\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on Fashion MNIST using the Leaky ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 9s 4ms/step - loss: 1.6314 - accuracy: 0.5054 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.8416 - accuracy: 0.7246 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.7053 - accuracy: 0.7637 - val_loss: 0.6427 - val_accuracy: 0.7902\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6325 - accuracy: 0.7907 - val_loss: 0.5900 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5992 - accuracy: 0.8019 - val_loss: 0.5582 - val_accuracy: 0.8202\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5624 - accuracy: 0.8142 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5379 - accuracy: 0.8218 - val_loss: 0.5157 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5152 - accuracy: 0.8294 - val_loss: 0.5079 - val_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5100 - accuracy: 0.8268 - val_loss: 0.4895 - val_accuracy: 0.8388\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4918 - accuracy: 0.8339 - val_loss: 0.4817 - val_accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try PReLU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ***PReLU***, replace ***LeakyRelu(alpha=0.2)*** with ***`PReLU()`***. There is currently no official implementation of RReLU in Keras, but you can fairly easily implement your own (to learn how to do that, see the exercises at the end of Chapter 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 300)               300       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 267,010\n",
      "Trainable params: 267,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7632\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7211 - accuracy: 0.7620 - val_loss: 0.6565 - val_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6448 - accuracy: 0.7880 - val_loss: 0.6003 - val_accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6078 - accuracy: 0.8003 - val_loss: 0.5656 - val_accuracy: 0.8180\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5693 - accuracy: 0.8117 - val_loss: 0.5406 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5428 - accuracy: 0.8193 - val_loss: 0.5196 - val_accuracy: 0.8312\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5193 - accuracy: 0.8284 - val_loss: 0.5113 - val_accuracy: 0.8318\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5129 - accuracy: 0.8274 - val_loss: 0.4916 - val_accuracy: 0.8378\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4941 - accuracy: 0.8313 - val_loss: 0.4826 - val_accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, a [2015 paper](https://homl.info/50) by Djork-Arné Clevert et al. proposed a new activation function called the **`exponential linear unit (ELU)`** that outperformed all the ReLU variants in the authors’ experiments: `training time was reduced, and the neural network performed better on the test set.` Figure 11-3(below) graphs the function, and Equation 11-2 shows its definition.\n",
    "\n",
    "*Equation 11-2. ELU activation function*\n",
    "\n",
    "**ELU**$_\\alpha (z) = \\alpha(exp(z)-1$ **`if`** $z < 0$, $z$ **`if`** $z>= 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure ELU_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.plot(z, elu(z), 'b-', linewidth=2)\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(r'ELU activation function ($\\alpha=1$)', fontsize=16)\n",
    "plt.axis([-4, 4, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"ELU_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELU activation function looks a lot like the ReLU function, with a few major differences:\n",
    "* It takes on negative values when $z < 0$, which allows the unit to have an average output closer to $0$ and helps alleviate the vanishing gradients problem. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter.\n",
    "\n",
    "* It has a nonzero gradient for $z < 0$, which avoids the dead neurons problem.\n",
    "\n",
    "* If $\\alpha$ is equal to $1$ then the function is smooth everywhere, including around $z = 0$, which helps speed up Gradient Descent since it does not bounce as much to the left and right of $z = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f0f611ac4e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a [2017 paper](https://homl.info/selu) by Günter Klambauer et al. introduced the `Scaled ELU (SELU)` activation function: as its name suggests, it is a scaled variant of the ELU activation function. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize: the output of each layer will tend to `preserve a mean of 0 and standard deviation of 1 during training`, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones). There are, however, a few conditions for self-normalization to happen (see the paper for the mathematical justification):\n",
    "\n",
    "* The input features must be standardized (mean 0 and standard deviation 1).\n",
    "\n",
    "* Every hidden layer’s weights must be initialized with LeCun normal initialization. In Keras, this means setting `kernel_initializer=\"lecun_normal\"`.\n",
    "\n",
    "* `The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks (see Chapter 15) or networks with skip connections (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.`\n",
    "\n",
    "* `The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well (see Chapter 14).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ***SELU*** activation, set ***`activation=\"selu\"`*** and ***`kernel_initializer=\"lecun_normal\"`*** when creating a layer:\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(10, activation='selu', \n",
    "                  kernel_initializer='lecun_normal')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f0f5e303eb8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='selu', \n",
    "                  kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***`In Colab notebook`***\n",
    "\n",
    "> This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, `this activation function outperforms the other activation functions very significantly for such neural nets`, so you should really try it out. `Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use` ℓ<sub>1</sub> `or` ℓ<sub>2</sub> `regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize).` ***However, in practice it works quite well with sequential CNNs.*** `If you break self-normalization, SELU will not necessarily outperform other activation functions.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the ***SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SELU is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f0f5fed5ef0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 35s 18ms/step - loss: 1.5515 - accuracy: 0.4109 - val_loss: 1.0267 - val_accuracy: 0.6228\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.8707 - accuracy: 0.6588 - val_loss: 0.7100 - val_accuracy: 0.7198\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.7340 - accuracy: 0.7023 - val_loss: 0.6401 - val_accuracy: 0.7432\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.6358 - accuracy: 0.7548 - val_loss: 0.5612 - val_accuracy: 0.8010\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.5677 - accuracy: 0.7932 - val_loss: 0.5534 - val_accuracy: 0.8102\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now look at what happens if we try to use the ReLU activation function instead:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\",\n",
    "                             kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\",\n",
    "                                 kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 33s 16ms/step - loss: 2.0794 - accuracy: 0.1857 - val_loss: 1.2931 - val_accuracy: 0.4426\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.2462 - accuracy: 0.4646 - val_loss: 0.8471 - val_accuracy: 0.6764\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 1.1994 - accuracy: 0.5252 - val_loss: 0.8759 - val_accuracy: 0.6806\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.9132 - accuracy: 0.6373 - val_loss: 0.7527 - val_accuracy: 0.7110\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 0.7970 - accuracy: 0.6848 - val_loss: 0.7049 - val_accuracy: 0.7310\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`Not great at all, we suffered from the vanishing/exploding gradients problem.`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIP\n",
    "\n",
    "> So, which activation function should you use for the hidden layers of your deep neural networks? Although your mileage will vary, in general **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic</span>**. If the network’s architecture prevents it from self-normalizing, then **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ELU</span>** may perform better than **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">SELU</span>** (since **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">SELU</span>** is not smooth at $z = 0$). If you care a lot about runtime latency, then you may prefer **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">leaky ReLU</span>**. If you don’t want to tweak yet another hyperparameter, you may use the default $α$ values used by Keras (e.g., $0.3$ for **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">leaky ReLU</span>**). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">RReLU</span>** if your network is overfitting or **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">PReLU</span>** if you have a huge training set. That said, because **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ReLU</span>** is the most used activation function (by far), many libraries and hardware accelerators provide **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ReLU</span>**-specific optimizations; therefore, if speed is your priority, **<span style= \"font-family: 'Bebas Neue'; font-size:1.2em\">ReLU</span>** might still be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although using _He_ initialization along with *ELU* (or any variant of *ReLU*) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [2015 paper](https://homl.info/51), Sergey Ioffe and Christian Szegedy proposed a technique called `Batch Normalization (BN) that addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”). The whole operation is summarized step by step in Equation 11-3.\n",
    "\n",
    "![Batch Normalization algorithm](images/training_DNN/batch_normalization_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm:\n",
    "\n",
    "* $μ_B$ is the vector of input means, evaluated over the whole mini-batch $B$ (it contains one mean per input).\n",
    "\n",
    "* $σ_B$ is the vector of input standard deviations, also evaluated over the whole mini-batch (it contains one standard deviation per input).\n",
    "\n",
    "* $m_B$ is the number of instances in the mini-batch.\n",
    "\n",
    "* $\\hat{x}^{(i)}$ is the vector of zero-centered and normalized inputs for instance $i$.\n",
    "\n",
    "* $γ$ is the output scale parameter vector for the layer (it contains one scale parameter per input).\n",
    "\n",
    "* $⊗$ represents element-wise multiplication (each input is multiplied by its corresponding output scale parameter).\n",
    "\n",
    "* $β$ is the output shift (offset) parameter vector for the layer (it contains one offset parameter per input). Each input is offset by its corresponding shift parameter.\n",
    "\n",
    "* $ε$ is a tiny number that avoids division by zero (typically $10^{–5}$). This is called a `smoothing` term.\n",
    "\n",
    "* $z^{(i)}$ is the output of the BN operation. It is a rescaled and shifted version of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it’s not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable. `One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions. However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer’s input means and standard deviations.` This is what Keras does automatically when you use the BatchNormalization layer. To sum up, four parameter vectors are learned in each batch-normalized layer: $\\gamma$ (the output scale vector) and $\\beta$ (the output offset vector) are learned through regular backpropagation, and μ (the final input mean vector) and $\\sigma$ (the final input standard deviation vector) are estimated using an exponential moving average. Note that $\\mu$ and $\\sigma$ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations in Equation 11-3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all the deep neural networks they experimented with, leading to a huge improvement in the ImageNet classification task (ImageNet is a large database of images classified into many classes, commonly used to evaluate computer vision systems). The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function. The networks were also much less sensitive to the weight initialization. The authors were able to use much larger learning rates, significantly speeding up the learning process. Specifically, they note that:\n",
    "\n",
    "> ***Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. […] Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Finally, like a gift that keeps on giving, Batch Normalization acts like a regularizer, reducing the need for other regularization techniques (such as dropout, described later in this chapter).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it’s often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. For example, if the previous layer computes $XW + b$, then the BN layer will compute $γ⊗(XW + b – μ)/σ + β$ (ignoring the smoothing term $ε$ in the denominator). If we define $W′ = γ⊗W/σ$ and $b′ = γ⊗(b – μ)/σ + β$, the equation simplifies to $XW′ + b′$. So if we replace the previous layer’s weights and biases ($W$ and $b$) with the updated weights and biases ($W′$ and $b′$), we can get rid of the BN layer (TFLite’s optimizer does this automatically; see Chapter 19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "> You may find that training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance. All in all, `wall time` will usually be shorter (this is the time measured by the clock on your wall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a BatchNormalization layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model. For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "#     keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s all! **`In this tiny example with just two hidden layers, it’s unlikely that Batch Normalization will have a very positive impact; but for deeper networks it can make a tremendous difference.`**\n",
    "\n",
    "Let’s display the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 334,346\n",
      "Trainable params: 331,578\n",
      "Non-trainable params: 2,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each BN layer adds four parameters per input: $γ$, $β$, $μ$, and $σ$ (for example, the first BN layer adds $3,136$ parameters, which is $4 × 784$). The last two parameters, $μ$ and $σ$, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable” (if you count the total number of BN parameters, $3,136 + 1,200 + 400$, and divide by $2$, you get $2,368$, which is the total number of non-trainable parameters in this model).\n",
    "\n",
    "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_3/gamma:0', True),\n",
       " ('batch_normalization_3/beta:0', True),\n",
       " ('batch_normalization_3/moving_mean:0', False),\n",
       " ('batch_normalization_3/moving_variance:0', False)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you create a BN layer in Keras, it also creates **`two operations` that will be called by Keras at each iteration during training. These operations will update the moving averages.** Since we are using the TensorFlow backend, these operations are TensorFlow operations (we will discuss TF operations in Chapter 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bn1.updates #deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 6ms/step - loss: 1.0867 - accuracy: 0.6377 - val_loss: 0.5053 - val_accuracy: 0.8268\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.5625 - accuracy: 0.8011 - val_loss: 0.4401 - val_accuracy: 0.8492\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.5007 - accuracy: 0.8237 - val_loss: 0.4117 - val_accuracy: 0.8588\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4586 - accuracy: 0.8373 - val_loss: 0.3937 - val_accuracy: 0.8612\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4427 - accuracy: 0.8436 - val_loss: 0.3822 - val_accuracy: 0.8648\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4132 - accuracy: 0.8521 - val_loss: 0.3730 - val_accuracy: 0.8704\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4003 - accuracy: 0.8578 - val_loss: 0.3646 - val_accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3829 - accuracy: 0.8647 - val_loss: 0.3613 - val_accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3808 - accuracy: 0.8637 - val_loss: 0.3541 - val_accuracy: 0.8736\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3642 - accuracy: 0.8692 - val_loss: 0.3505 - val_accuracy: 0.8744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "keras.layers.BatchNormalization(),\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", \n",
    "                             kernel_initializer='he_normal'))\n",
    "keras.layers.BatchNormalization(), # commented when testing on without BN layer\n",
    "\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", \n",
    "                                kernel_initializer='he_normal'))\n",
    "    keras.layers.BatchNormalization(), # commented when testing on without BN layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 30s 16ms/step - loss: 2.3026 - accuracy: 0.1004 - val_loss: 2.3026 - val_accuracy: 0.0914\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 2.3026 - accuracy: 0.0992 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 2.3026 - accuracy: 0.0985 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 2.3026 - accuracy: 0.1030 - val_loss: 2.3027 - val_accuracy: 0.0914\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3027 - val_accuracy: 0.0914\n"
     ]
    }
   ],
   "source": [
    "# with batch normalization layer + when kernel_initializer not given\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 33s 17ms/step - loss: 2.0873 - accuracy: 0.2029 - val_loss: 1.7030 - val_accuracy: 0.3480\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 1.7249 - accuracy: 0.3299 - val_loss: 1.3305 - val_accuracy: 0.4888\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 27s 15ms/step - loss: 1.3244 - accuracy: 0.4778 - val_loss: 1.3690 - val_accuracy: 0.4374\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 1.0806 - accuracy: 0.5655 - val_loss: 0.8215 - val_accuracy: 0.6950\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.0400 - accuracy: 0.5943 - val_loss: 1.2865 - val_accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "# with batch normalization layer with kernel_initializer='he_normal'\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 31s 16ms/step - loss: 2.0873 - accuracy: 0.2029 - val_loss: 1.7030 - val_accuracy: 0.3480\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.7249 - accuracy: 0.3299 - val_loss: 1.3305 - val_accuracy: 0.4888\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.3244 - accuracy: 0.4778 - val_loss: 1.3690 - val_accuracy: 0.4374\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.0806 - accuracy: 0.5655 - val_loss: 0.8215 - val_accuracy: 0.6950\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 1.0400 - accuracy: 0.5943 - val_loss: 1.2865 - val_accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "# without batch normalization layer expect after input layer  with kernel_initializer='he_normal'\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after (as we just did). There is some debate about this, as which is preferable seems to depend on the task—you can experiment with this too to see which option works best on your dataset. To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass use_bias=False when creating it):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
